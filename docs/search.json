[
  {
    "objectID": "week6_literature.html",
    "href": "week6_literature.html",
    "title": "Literature",
    "section": "",
    "text": "Please read the below article. Use the reading aid and reading questions to help you get a better understanding of the main points of the text.",
    "crumbs": [
      "Week 6: Estimating Causal Effects of Time-Varying Exposures",
      "Literature"
    ]
  },
  {
    "objectID": "week6_literature.html#reading-aid",
    "href": "week6_literature.html#reading-aid",
    "title": "Literature",
    "section": "Reading Aid",
    "text": "Reading Aid",
    "crumbs": [
      "Week 6: Estimating Causal Effects of Time-Varying Exposures",
      "Literature"
    ]
  },
  {
    "objectID": "week6_lab.html",
    "href": "week6_lab.html",
    "title": "Lab Exercises",
    "section": "",
    "text": "In this lab, you practice with Phase III of a causal research project, specifically estimating causal effects of time-varying exposures using inverse probability weighting (IPW)-estimation of the parameters in a marginal structural model (MSM). To this end, we will be using simulated data again.\nWe will investigate the joint effect of a time-varying binary exposure (measured at three occasions) on a continuous distal outcome. Examples of this are the joint effect of\nIn all these cases, the exposure is a binary variable that may vary over time, and that may affect the final outcome at the end of the study directly or indirectly through shaping earlier realizations of the outcome variable."
  },
  {
    "objectID": "week6_lab.html#structural-relations-truth",
    "href": "week6_lab.html#structural-relations-truth",
    "title": "Lab Exercises",
    "section": "Structural Relations (truth)",
    "text": "Structural Relations (truth)\nConsider the R code below that we will use to simulate data.\n\nN &lt;- 1000000\nC &lt;- rnorm(N)\n\ndata &lt;- data.frame(C)\n\n# Simulate data for first wave\ndata$Y1 &lt;- .4*C + rnorm(N)\ndata$X1 &lt;- rbinom(n = N, size=1, prob = plogis(C))\n\n# Simulate data for the second wave\ndata$Y2 &lt;- 0.1 * data$Y1 + 0.3 * data$X1 + rnorm(N)\ndata$X2 &lt;- rbinom(n = N, size=1, prob = \n        plogis(-.8 + 0.2 * data$Y1 + 1 * data$X1))\n\n# Simulate data for the third wave\ndata$Y3 &lt;- 0.1 * data$Y2 + 0.3 * data$X2 + \n        0.8 * data$Y1 + 0.15 * data$X1 + rnorm(N)\ndata$X3 &lt;- rbinom(n = N, size=1, prob = \n        plogis(-.8 + 0.2 * data$Y2 + 1 * data$X2 +\n            0.1 * data$Y1 + 0.8 * data$X1))\n\n# Simulate the final outcome\ndata$final.Y &lt;- 0.1 * data$Y3 + 0.3 * data$X3 + 0.8 * data$Y2 + \n            0.15 * data$X2 + rnorm(N)\n\n# Check the data file\nhead(data)\n\n           C         Y1 X1         Y2 X2         Y3 X3    final.Y\n1 -0.9859817 -1.5564531  1  0.4207018  1 -1.2906857  1 -0.1626745\n2 -0.8954917 -0.1590391  0  0.9964082  1 -0.6715871  1  1.6980897\n3 -1.4174217 -1.6120160  0  0.2818158  0 -1.4942573  0 -0.3980178\n4 -0.3514811 -0.4071577  1  0.7485201  0 -0.8469769  1  0.8747929\n5  0.9270485  0.8132253  1  0.5070488  1  1.6371660  0 -0.0507478\n6 -0.5328835 -1.4577802  0 -0.9812450  1 -0.5578404  1 -0.9695687\n\n\n\n\n\n\n\n\nDraw the causal DAG that is encode in the R code above.\n\n\n\n\n\n\n\n\nCausal DAG based on the R code.\n\n\nThis figure does not depict linear or logistic regressions per se, but instead visualizes the causal (structural) relations between the variables. However, since we generated the data ourselves, we know that these relationships are in fact linear and logistic.\n\n\n\n\n\n\n\n\n\nIndicate which causal paths there are from \\(X_{1}\\) to \\(Y_{4}\\) and that do not go through later exposures. Based on the population parameter values in the R code to generate the data, compute what the true controlled direct effect is of \\(X_{1}\\) on \\(Y_{4}\\).\n\n\n\n\n\n\n# X1 -&gt; Y2 -&gt; Y3 -&gt; Y.final\n0.3*0.1*0.1\n\n[1] 0.003\n\n# X1 -&gt; Y2 -&gt; Y.final\n0.3*0.8\n\n[1] 0.24\n\n# X1 -&gt; Y3 -&gt; Y.final\n0.15*0.1\n\n[1] 0.015\n\n# Total CDE of X1 on Y4\n0.3*0.1*0.1 + 0.3*0.8 + 0.15*0.1\n\n[1] 0.258\n\n\n\n\n\n\n\n\n\n\n\nIndicate which causal paths there are from \\(X_{2}\\) to \\(Y_{4}\\) and that do not go through later exposures. Based on the population parameter values in the R code to generate the data, compute what the true controlled direct effect is of \\(X_{2}\\) on \\(Y_{4}\\).\n\n\n\n\n\n\n# X2 -&gt; Y3 -&gt; Y.final\n0.3*0.1\n\n[1] 0.03\n\n# X2 -&gt; Y.final\n0.15\n\n[1] 0.15\n\n# Total CDE of X2 on Y4\n0.3*0.1 + 0.15\n\n[1] 0.18\n\n\n\n\n\n\n\n\n\n\n\nIndicate which causal paths there are from \\(X_{3}\\) to \\(Y_{4}\\) and that do not go through later exposures. Based on the population parameter values in the R code to generate the data, compute what the true controlled direct effect is of \\(X_{3}\\) on \\(Y_{4}\\).\n\n\n\n\n\n\n# X3 -&gt; Y.final\n0.3\n\n[1] 0.3\n\n# Total CDE X3 on Y4\n0.3\n\n[1] 0.3"
  },
  {
    "objectID": "week6_lab.html#standard-linear-regression",
    "href": "week6_lab.html#standard-linear-regression",
    "title": "Lab Exercises",
    "section": "Standard linear regression",
    "text": "Standard linear regression\nWe want to estimate the effect of exposure over occasions 1 to 3 on an end-of-study outcome \\(Y_{4}\\). The previous measures of \\(Y\\) can be regarded as time-varying covariates.\n\n\n\n\n\n\nFor \\(Y_{1}\\), \\(Y_{2}\\), and \\(Y_{3}\\), discuss whether one should control for them, or not.\n\n\n\n\n\nVariable \\(Y_{1}\\) is only a confounder between exposure and the outcome; hence, we should control for it.\nThe variable \\(Y_{2}\\) has two roles in the causal structure. It is a confounder of the relation between \\(X_{3}\\) and \\(Y_{4}\\): \\(X_{3} \\leftarrow Y_{2} \\rightarrow Y_{3} \\rightarrow Y_{4}\\). This would imply we should control for it to avoid confounder bias. It is also a mediator for the effect of \\(X_{1}\\) on \\(Y_{4}\\): \\(X_{1} \\rightarrow Y_{2} \\rightarrow Y_{3} \\rightarrow Y_{4}\\) and \\(X_{1} \\rightarrow Y_{2} \\rightarrow X_{3} \\rightarrow Y_{4}\\). This would imply we should not control for it, to avoid overcontrol bias (that is, bias that arises when we block an indirect path from the cause to the effect). Controlling for \\(Y_{2}\\) would remove part of the total causal effect \\(X_{1}\\) has on \\(Y_{4}\\). This is a catch-22.\nThe variable \\(Y_{3}\\) is only a mediator in this model: \\(X_{1} \\rightarrow Y_{2} \\rightarrow Y_{3} \\rightarrow Y_{4}\\); \\(X_{1} \\rightarrow X_2 \\rightarrow Y_{3} \\rightarrow Y_{4}\\); \\(X_{1} \\rightarrow Y_{3} \\rightarrow Y_{4}\\); \\(X_{2} \\rightarrow Y_{3} \\rightarrow Y_{4}\\). So, including \\(Y_{3}\\) as a covariate will lead to overcontrol bias in estimating the controlled direct effects of \\(X_{1}\\) and \\(X_{2}\\).\n\n\n\nBefore we consider a more sophisticated approach to this problem, we begin with considering two simpler models. Each of these is associated with a specific form of bias in estimating the causal effect of the time-varying exposure on \\(Y_{4}\\).\n\n\n\n\n\n\nRun a regression model with the time-varying covariates included.\n\n\n\n\n\n\nout1 &lt;- glm(final.Y ~ C + X1 + X2 + X3 + Y1 + Y2 + Y3, data=data)\nsummary(out1)\n\n\nCall:\nglm(formula = final.Y ~ C + X1 + X2 + X3 + Y1 + Y2 + Y3, data = data)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.355e-03  1.761e-03   1.338   0.1810    \nC            9.457e-05  1.169e-03   0.081   0.9355    \nX1          -4.598e-03  2.314e-03  -1.988   0.0469 *  \nX2           1.490e-01  2.178e-03  68.428   &lt;2e-16 ***\nX3           2.995e-01  2.148e-03 139.451   &lt;2e-16 ***\nY1          -2.228e-04  1.290e-03  -0.173   0.8628    \nY2           8.009e-01  1.010e-03 793.248   &lt;2e-16 ***\nY3           9.965e-02  1.001e-03  99.585   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 1.001458)\n\n    Null deviance: 1807375  on 999999  degrees of freedom\nResidual deviance: 1001450  on 999992  degrees of freedom\nAIC: 2839344\n\nNumber of Fisher Scoring iterations: 2\n\n\nIn this model, we have the problem of over control bias due to controlling for mediators \\(Y_{2}\\) and \\(Y_{3}\\), which blocks causal paths from the time-varying exposure to the outcome.\n\n\n\n\n\n\n\n\n\nRun a regression model without the time-varying covariates.\n\n\n\n\n\n\nout2 &lt;- glm(final.Y ~ C + X1 + X2 + X3 + Y1, data=data)\nsummary(out2)\n\n\nCall:\nglm(formula = final.Y ~ C + X1 + X2 + X3 + Y1, data = data)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.044808   0.002269 -19.752   &lt;2e-16 ***\nC            0.001731   0.001507   1.149    0.251    \nX1           0.220371   0.002957  74.527   &lt;2e-16 ***\nX2           0.142184   0.002780  51.148   &lt;2e-16 ***\nX3           0.459579   0.002757 166.696   &lt;2e-16 ***\nY1           0.156287   0.001298 120.426   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 1.66392)\n\n    Null deviance: 1807375  on 999999  degrees of freedom\nResidual deviance: 1663910  on 999994  degrees of freedom\nAIC: 3347061\n\nNumber of Fisher Scoring iterations: 2\n\n\nThis model results in confounder bias because it fails to control for confounders \\(Y_{1}\\) and \\(Y_{2}\\). Note that \\(Y_{3}\\) is not a counfounder in this model, so omitting it is not associated with confounder bias.\n\n\n\n\n\n\n\n\n\nCompare the results of these two models.\n\n\n\n\n\nThe results for the effect of \\(X_1\\) to \\(X_3\\) on \\(Y_{4}\\) are quite different across these two models, and they also deviate from the actual effects that we computed based on the truth above:\n\n\n\nExposure\nModel 1\nModel 2\nTruth\n\n\n\n\n\\(X_{1}\\)\n-0.00\n0.22\n0.258\n\n\n\\(X_{2}\\)\n0.15\n0.14\n0.180\n\n\n\\(X_{3}\\)\n0.30\n0.46\n0.300\n\n\n\nWe know that neither model is correct: Model 1 blocks causal paths through \\(Y_2\\) and \\(Y_3\\) (overcontrol bias), while model 2 fails to account for confounding due to \\(Y_1\\) and \\(Y_2\\)."
  },
  {
    "objectID": "week6_lab.html#ipw-estimation",
    "href": "week6_lab.html#ipw-estimation",
    "title": "Lab Exercises",
    "section": "IPW estimation",
    "text": "IPW estimation\nWe will now use the marginal structural model as described by VanderWeele, Jackson, and Li (2016).\n\n\n\n\n\n\nFirst, compute the propensity score (i.e., the probability of receiving exposure) at wave 1, wave 2, and 3 using logistic regression. For each of these, you should include all prior versions of \\(X\\) and the covariate \\(Y\\), and all time-invariant (or baseline) covariates (here \\(C\\)).\n\n\n\n\n\n\n# Compute the propensity scores at wave 1 \nres.X1 &lt;- glm(X1 ~  C, family = binomial(), data = data)\nps1 &lt;- predict(res.X1, type = \"response\")\ndata$ps1 &lt;- ps1\n\n# Compute the propensity scores at wave 2 \nres.X2 &lt;- glm(X2 ~  C + X1 + Y1, family = binomial(), data = data)\nps2 &lt;- predict(res.X2, type = \"response\")\ndata$ps2 &lt;- ps2\n\n# Compute the propensity scores at wave 3 \nres.X3 &lt;- glm(X3 ~ C + X1 + X2 + Y1 + Y2, family = binomial(), data = data)\nps3 &lt;- predict(res.X3, type = \"response\")\ndata$ps3 &lt;- ps3\n\n\n\n\nNote that strictly speaking, for this particular model (as shown in the DAG), we would not need to include C to estimate the propensity scores at wave 2 and wave 3.\n\n\n\n\n\n\nExplain why not, and whether it matters that we include it here.\n\n\n\n\n\n\\(C\\) only has direct effects on X1 and Y1; since both are included as predictors for the propensity scores at wave 2 and wave 3, the effect of \\(C\\) is already controlled for then. However, it does not create a problem to include it; it does not block a indirect path that we would want to remain open (i.e., it is not a mediator), nor does it open a path that should remain closed (i.e., it is not a collider). Including or not including it for ps2 and ps3 does not make a difference.\n\n\n\n\n\n\n\n\n\nMake histograms for the propensity scores of the treated and the untreated at wave 2 and wave 3. What does this show?\n\n\n\n\n\n\n# Plot the propensity scores at each wave\nM&lt;-matrix(c(1:3),1,3, byrow = FALSE)\nlayout(M)\n\nfor (t in 1:3)\n{   k &lt;- subset(data, select = c(paste0(\"X\", t)))\n    data.1 &lt;- data[ which(k == 1), ]\n    data.0 &lt;- data[ which(k == 0), ]\n    ps.t.1 &lt;- subset(data.1, select = c(paste0(\"ps\", t)))\n    ps.t.0 &lt;- subset(data.0, select = c(paste0(\"ps\", t)))\n    hist0 &lt;- hist(as.numeric(ps.t.1[[1]]), breaks=30, plot=FALSE)\n    hist1 &lt;- hist(ps.t.0[[1]], breaks=30, plot=FALSE)\n    title &lt;- paste0(\"Propensity scores at wave \", t)\n    plot( hist1, col=rgb(0,0,1,1/4), xlim=c(0,1), \n            xlab=\"Propensity score\", main=title)  \n    plot( hist0, col=rgb(0,1,0,1/4), xlim=c(0,1), add=T) \n}\n\n\n\n\n\n\n\n\nIt shows that the distribution of the propensity scores for the treated and the untreated overlap well (assumption of positivity), at each occasion.\n\n\n\nRecall that the unstabilized inverse probability weights at a specific time point are computed as \\[\nW_{it} = X_{it} \\frac{1}{P[X_{it} = 1 | \\bar{L}_{it}, \\bar{A}_{i,t - 1}]} + (1 - X_{it})\\frac{1}{(1 - P[X_{it} = 1 | \\bar{L}_{it}, \\bar{A}_{i,t - 1}])}.\n\\] From these wave-specific weights, compute the overall weight, by taking the product of the wave-specific weights.\n\n\n\n\n\n\nCompute the weights.\n\n\n\n\n\n\n# Compute the inverse probability weights at waves 1, 2 and 3\ndata$ipw1 &lt;- ifelse(data$X1 == 1, 1 / data$ps1, 1 / (1 - data$ps1))\ndata$ipw2 &lt;- ifelse(data$X2 == 1, 1 / data$ps2, 1 / (1 - data$ps2))\ndata$ipw3 &lt;- ifelse(data$X3 == 1, 1 / data$ps3, 1 / (1 - data$ps3))\n\n# Compute the total ipw\ndata$ipw.123 &lt;- data$ipw1 * data$ipw2 * data$ipw3 \n\n\n\n\nFinally, we can run a regression model with \\(Y_{4}\\) as the outcome variable, and \\(X_1\\), \\(X_2\\), \\(X_3\\), and \\(Y_1\\) as its predictors, using the total weights computed above.\n\n\n\n\n\n\nEstimate the parameters of the MSM.\n\n\n\n\n\n\n# Regression with inverse probability weighting with X1, X2, X3 and Y1 as its predictors\nout3 &lt;- glm(final.Y ~ X1 + X2 + X3 + Y1, \n        weights = ipw.123, data=data)\n\n# Another regression with inverse probability weighting which now also includes the baseline confounder (just \n# for comparison)\nout4 &lt;- glm(final.Y ~ C + X1 + X2 + X3 + Y1, \n        weights = ipw.123, data=data)\n\n# Another regression with inverse probability weighting which includes C but not Y1\nout5 &lt;- glm(final.Y ~ C + X1 + X2 + X3, \n        weights = ipw.123, data=data)\n\nsummary(out3)\n\n\nCall:\nglm(formula = final.Y ~ X1 + X2 + X3 + Y1, data = data, weights = ipw.123)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.004579   0.002587    1.77   0.0767 .  \nX1          0.252019   0.002587   97.43   &lt;2e-16 ***\nX2          0.182161   0.002587   70.42   &lt;2e-16 ***\nX3          0.299724   0.002587  115.87   &lt;2e-16 ***\nY1          0.160831   0.001199  134.12   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 13.38084)\n\n    Null deviance: 13994881  on 999999  degrees of freedom\nResidual deviance: 13380771  on 999995  degrees of freedom\nAIC: 3562727\n\nNumber of Fisher Scoring iterations: 2\n\nsummary(out4)\n\n\nCall:\nglm(formula = final.Y ~ C + X1 + X2 + X3 + Y1, data = data, weights = ipw.123)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.004581   0.002587   1.771   0.0766 .  \nC           0.001506   0.001392   1.082   0.2791    \nX1          0.252019   0.002587  97.430   &lt;2e-16 ***\nX2          0.182161   0.002587  70.423   &lt;2e-16 ***\nX3          0.299723   0.002587 115.872   &lt;2e-16 ***\nY1          0.160309   0.001293 124.027   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 13.38084)\n\n    Null deviance: 13994881  on 999999  degrees of freedom\nResidual deviance: 13380755  on 999994  degrees of freedom\nAIC: 3562728\n\nNumber of Fisher Scoring iterations: 2\n\nsummary(out5)\n\n\nCall:\nglm(formula = final.Y ~ C + X1 + X2 + X3, data = data, weights = ipw.123)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.004645   0.002607   1.782   0.0747 .  \nC           0.065927   0.001301  50.669   &lt;2e-16 ***\nX1          0.252134   0.002606  96.733   &lt;2e-16 ***\nX2          0.181886   0.002606  69.782   &lt;2e-16 ***\nX3          0.299968   0.002606 115.085   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 13.58666)\n\n    Null deviance: 13994881  on 999999  degrees of freedom\nResidual deviance: 13586588  on 999995  degrees of freedom\nAIC: 3577991\n\nNumber of Fisher Scoring iterations: 2\n\n\nIt shows that whether the baseline covariate \\(C\\) is included or not, makes no difference. This is because it has already been accounted for when computing the inverse probability weights. Furthermore, when considering the parameter estimates here it can be seen these are in fact very close to the true effects, which were computed at the start based on the parameter values that were used to simulate the data (i.e., the Truth)."
  },
  {
    "objectID": "week6_lab.html#conclusion",
    "href": "week6_lab.html#conclusion",
    "title": "Lab Exercises",
    "section": "Conclusion",
    "text": "Conclusion\nCompare the results from the marginal structural model to the results from the other two models.\n\n\n\n\n\n\nWhat does this show you?\n\n\n\n\n\n\n\n\nExposure\nModel 1\nModel 2\nModel 3\nTruth\n\n\n\n\n\\(X_{1}\\)\n-0.00\n0.22\n0.26\n0.258\n\n\n\\(X_{2}\\)\n0.15\n0.14\n0.18\n0.180\n\n\n\\(X_{3}\\)\n0.30\n0.46\n0.30\n0.300\n\n\n\nModel 1 includes \\(Y_2\\) and \\(Y_3\\) as a covariate, and thus leads to overcontrol bias when estimating the effects of \\(X_1\\) and \\(X_2\\). It only shows the direct effects of the time-varying exposure, not the indirect effects.\nModel 2 does not include \\(Y_2\\) and \\(Y_3\\) in any manner. Therefore, this model leads to confounder bias, since \\(Y_2\\) is a confounder for the \\(X_3 \\rightarrow Y_{4}\\) relation.\nModel 3 is used to account for confounders, without blocking indirect paths. Hence, it should inform us on the joint exposure effect of the time-varying exposure. Its estimates are very close to the true values (based on the model parameters that we used to simulated the data with).\n\n\n\nWhy IPW estimation of an MSM works, and how it accounts for time-dependent confounding without blocking the relevant mediation paths, is not easy to see, or to even get some intuition for. But recall that in week 2 we learned that by using IPW, we create a balanced sample (also sometimes referred to as a pseudo population) that, within a certain level of the confounder, has an equal number of individuals in each exposure group. That implied that in this balance sample, we have \\(P[X_{it} = 1] = 0.5\\) for everyone. This balancing property of IPW is therefore a way to mimic an RCT. Balancing thus removes the arrows that point into the exposure nodes (again, as would be the case in an RCT)."
  },
  {
    "objectID": "week6_lab.html#some-useful-r-packages",
    "href": "week6_lab.html#some-useful-r-packages",
    "title": "Lab Exercises",
    "section": "Some useful R packages",
    "text": "Some useful R packages\nFor these exercises we have made use of base R functions. This is helpful to get a better understanding of how IPW estimation actually works. However, there exist many useful packages that can help with assessing covariate (im)balance, and with creating inverse probability weights. I highly recommend the packages cobalt and WeightIt by Noah Greifer, as they have excellent documentation online that can help you use more advanced IPW-related techniques. You might consider using these packages for assignment 2."
  },
  {
    "objectID": "week5_lab_noAnswers.html",
    "href": "week5_lab_noAnswers.html",
    "title": "Lab Exercises",
    "section": "",
    "text": "The exercises in this lab will guide you through the first two phases of causal inference when studying a time-varying exposure. However, in contrast to the lectures, you will (a) work with the empirical example in VanderWeele, Jackson, and Li (2016); (b) consider a more complex (and also more realistic) causal DAG, which will influence our decisions across the phases; and (c) work with simulated data to get a better understanding of what joint effects are, and how inverse probability weighting estimation works.\nThis week is also provides important input for your assignment. Please make sure that by the end of this lab, you have read the assignment. That way, you have the opportunity to ask for clarification if anything is unclear.\n\nPhase I: Formulation\nVanderWeele, Jackson, and Li (2016) use an empirical example to illustrate the use of marginal structural models (MSMs) and inverse-probability-weighting (IPW) estimation. Re-read the section ``Results: empirical illustrations’’, and answer the below questions. Additionally, look up the empirical study upon which this example is based; You need this study to find answers to some of the questions. Finally, and perhaps most importantly, challenge yourself. Really try to come up with a comprehensive answer to the questions before you look at the answers. That way, you test if you truly grasp the contents, and whether or not you can apply it to new problems.\n\n\n\n\n\n\nFormulate the causal research question of VanderWeele, Jackson, and Li (2016) in words. In your answer, specify the target population, the exposures, the exposure contrast, and the outcome measure. Do VanderWeele, Jackson, and Li (2016) focus on specific regimes?\n\n\n\n\n\nGenerally speaking, VanderWeele, Jackson, and Li (2016) investigate the bidirectional relationship between religious service attendance and depression. More specifically, they assess the joint effect of religious service attendance in 1996 and in 2000 on depression in 2004; and the joint effect of depression in 1996 and 2000 on religious service attendance in 2004. For the rest of this exercise, we focus on religious service attendance as the exposure, and depression as the outcome. More information can be found in the study by Li et al. (2016).\nBoth VanderWeele, Jackson, and Li (2016) and Li et al. (2016) are implicit about the target population. Based on their sample, it can be inferred that their target population is female nurses across the United States. Li et al. (2016) states: “We, therefore, considered analyses both with and without participants who had a diagnosis of cardiovascular disease or cancer at baseline (n=19,803)”. They thus considered restricting their target population to only those without diagnosis of cardiovascular disease or cancer. Later, Li et al. (2016) describes how the analyses are repeated, but restricted to either only Catholics or Protestants, thereby further specifying a specific target population.\nThe outcome, depression, was defined as ``(…) either self-reported physician or clinician-diagnosed depression, or use of antidepressant medications, or depressive symptoms CESD-10 measure above 10.’’. The CESD-10 is a 10-item Likert scale screening questionnaire assessing depressive symptoms in the past week. It was measured in 2004, that is eight years after the first exposure-measurement (in 1996), and four years after the second exposure-measurement (in 2000).\nThe exposure was self-reported service attendance using the question: “How often in the past year do you attend religious services?” Answer categories were “More than once per week”, “Once per week”, “Less than once per week”, and “Never or almost never”.\nVanderWeele, Jackson, and Li (2016) and Li et al. (2016) do not express interest in any particular exposure regime. Therefore, there is also no explicit exposure contrast.\n\n\n\n\n\n\n\n\n\nIs this research question about an ACE, ACE\\(_{1}\\), or ACE\\(_{0}\\)? Reformulate the research question for those ACE\\(_{...}\\)’s left over.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFormulate the marginal structural model that is used. Tip: Consider how many times the exposure was measured, and the measurement level of the exposure and outcome; also see Table 2 in VanderWeele, Jackson, and Li (2016).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIs this a saturated MSM? If yes, what is the advantage/disadvantage of a saturated MSM? If no, what is the advantage/disadvantage of an unsaturated MSM? Tip: Think about the expected potential outcomes that exist, and compare this to the number of parameters in the MSM.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nLi, Shanshan, Olivia I Okereke, Shun-Chiao Chang, Ichiro Kawachi, and Tyler J. VanderWeele. 2016. “Religious Service Attendance and Lower Depression Among Women: A Prospective Cohort Study.” Annals of Behavioral Medicine 50 (6): 876–84. https://doi.org/10.1007/s12160-016-9813-9.\n\n\nVanderWeele, Tyler J., John W. Jackson, and Shanshan Li. 2016. “Causal Inference and Longitudinal Data: A Case Study of Religion and Mental Health.” Social Psychiatry and Psychiatric Epidemiology 51 (11): 1457–66. https://doi.org/10.1007/s00127-016-1281-9.",
    "crumbs": [
      "Week 5: MSMs and Time-Varying Exposures",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week4_literature.html",
    "href": "week4_literature.html",
    "title": "Literature",
    "section": "",
    "text": "Please read the second part of the below article, specifically the sections “Introduction to Propensity Scores” and “Discussion” (you can skip the section “Dual-Modeling Strategies”). Use the reading aid and reading questions to help you get a better understanding of the main points of the text.",
    "crumbs": [
      "Week 4: Rubin Causal Model (Part II)",
      "Literature"
    ]
  },
  {
    "objectID": "week4_literature.html#reading-aid",
    "href": "week4_literature.html#reading-aid",
    "title": "Literature",
    "section": "Reading Aid",
    "text": "Reading Aid\nThis week we continue our treatment of Schafer and Kang (2008), now focusing on Methods 4 to 6 which center around the use of propensity scores. Use the reading questions below to help you understand the main points of each section.\n\n“Introduction to Propensity Scores”\n\nOn page 294, it is mentioned that propensity score-based analysis techniques are “(…) in part a reaction to misapplications of ANCOVA, where analysts were often unaware of the sensitivity of their results to model failure.” What is meant by this?\nOn page 294, Schafer and Kang (2008) mention that “Girls who consider themselves overweight were more likely to diet.”. Based on this information, would a girl who considers herself overweight receive a relatively low or a relatively high propensity score?\nWhich analysis technique was used to predict the propensity scores? What is the outcome in this analysis, and what are the predictors?\nSuppose that we do a randomized controlled trial, and then compute propensity scores for treated and untreated based on all kinds of background variables. What can you say about the distribution of propensity scores in both groups in this context?\n\n\n\n“Using Propensity Scores to Estimate ACEs”\nThis might be your first encounter with methods such as matching, weighting, and subclassification. Try to understand as much as possible from the text and write down questions that you have. We cover this topic in detail during the lecture and the lab exercises.\n\n\n“Discussion”\nThe goal of an analysis is twofold (from a statistical perspective): (a) it should result in estimates that are as close as possible to the Truth, and (b) it should make efficient use of the data (i.e., you want small standard errors).\n\nLook at Table 6, which lists the estimated ACE for Methods 1 to 9. Ignoring Methods 7 to 9, which method came closest to the correct ACE for this one particular dataset?\nWhich method made the least efficient use of the data? Which explanation do Schafer and Kang (2008) give for this?",
    "crumbs": [
      "Week 4: Rubin Causal Model (Part II)",
      "Literature"
    ]
  },
  {
    "objectID": "week4_lab.html",
    "href": "week4_lab.html",
    "title": "Lab Exercises",
    "section": "",
    "text": "In these exercises, we continue with the analyses discussed in Schafer and Kang (2008), focusing on Methods 4 to 6."
  },
  {
    "objectID": "week4_lab.html#setup",
    "href": "week4_lab.html#setup",
    "title": "Lab Exercises",
    "section": "Setup",
    "text": "Setup\nIn this practical you will make use of various R packages. If you haven’t already, install the packages tableone, MatchIt, and survey using install.packages(c(\"tableone\", \"MatchIt\", \"survey\")).\nLoad the data, which are in the data file called SchaferKangData.dat. Take a look at the data set. See Table 3 in Schafer and Kang (2008) for a description of the variables.\n\ndf &lt;- read.table(\"SchaferKangData.dat\", header = TRUE)\nhead(df, n = 10)\n\nThere are various alternative techniques that are all based on using the propensity scores: This is an individual’s probability of treatment based on their scores on the covariates, \\(P(X=1|Z=Z)\\). If we know how likely a person was to receive treatment, we can use this information to mimic a randomized controlled trial (in which everyone has the same probability of receiving treatment). Schafer and Kang (2008) consider three common techniques for this:\n\nMethod 4: Matching, in which we try to create pairs of a treated and an untreated individual that have the same propensity score.\nMethod 5: Inverse probability weighting, in which we create a pseudo-population that is balanced on the covariates.\nMethod 6: Subclassification or stratification, in which we create strata in which there are no (meaningful) differences in the covariates left.\n\nRegardless of the choise of method, we need to estimate propensity scores first."
  },
  {
    "objectID": "week4_lab.html#estimate-propensity-scores",
    "href": "week4_lab.html#estimate-propensity-scores",
    "title": "Lab Exercises",
    "section": "Estimate Propensity Scores",
    "text": "Estimate Propensity Scores\nTo compute a propensity score, run a logistic regression model in which the treatment variable \\(X\\) (which has values 0 and 1) is the outcome variable, and the covariates are the predictors. Make sure to save the probability for each person for scoring 1 on X (here DIET). You can use the glm() function from the stats package for this, setting the family argument to family = binomial().\n\n\n\n\n\n\nLogistic Propensity Score Model\n\n\n\n\n\n\n# Run logistic regression analysis\nmod_logistic &lt;- glm(\n  formula = DIET ~ DISTR.1 + as.factor(BLACK) + as.factor(NBHISP) + GRADE + \n    SLFHLTH + SLFWGHT + WORKHARD + GOODQUAL + PHYSFIT + PROUD + LIKESLF + \n    ACCEPTED + FEELLOVD,\n  family = binomial(), \n  data = df\n)\n\n# Obtain a prediction of the probability of treatment (i.e., DIET = 1) \nps &lt;- predict(mod_logistic, type = \"response\")\n\n# Add this predicted probability to the data file\ndf$ps &lt;- ps\n\n# Look at the data file \nround(df[1:10,], 2)\n\nThe last column in the data file now contains the predicted probability of being treated, based on the covariates.\n\n\n\nNow that we have the propensity scores, we should first consider the distribution of propensity scores in each of the treatment groups separately, to determine whether there is overlap between the propensity scores of the two groups.\n\n\n\n\n\n\nMake a histogram to look at this, discuss what you see, and why this is important.\n\n\n\n\n\n\n# Subset datafile again based on treatment group\ndf1 &lt;- df[ which(df$DIET == 1), ]\ndf0 &lt;- df[ which(df$DIET == 0), ]\n\n# Generate histograms without plotting\nhist0 &lt;- hist(df0$ps, breaks = 30, plot = FALSE)\nhist1 &lt;- hist(df1$ps, breaks = 30, plot = FALSE)\n\n# Plot histograms\nplot( hist0, col=rgb(0,0,1,1/4), xlim=c(0,1), \n        xlab=\"Propensity score\", \n      main=\"Histogram of propensity scores\")  \nplot( hist1, col=rgb(1,0,0,1/4), xlim=c(0,1), add=T) \n\nWhat we see is that the distributions of propensity scores of the two groups seem to overlap well, even in the tails. If this would not be the case, that would be an indication that the causal identification assumption of positivity is violated. That is, at each possible combination of the covariates, which is now summarized with the propensity score, there should be both treated and non-treated individuals in our sample.\n\n\n\nNow that we have determined the propensity scores and their distributions for the the treated and the non-treated overlap well, we can make use of these scores within different techniques."
  },
  {
    "objectID": "week4_lab.html#method-4-matching",
    "href": "week4_lab.html#method-4-matching",
    "title": "Lab Exercises",
    "section": "Method 4: Matching",
    "text": "Method 4: Matching\nMatching based on the propensity scores is the first technique in this category that we consider. It is based on finding people in the both treatment groups that have similar propensity scores: The idea is that these individuals are comparable on the entire set of covariates, and can thus be considered randomly assigned to the two condition.\nIn practice, this is done by taking a person from the smallest of the two groups (here the group for which DIET=1), and finding a person in the other group that is most like this person in terms of their propensity score. We can do this using the function matchit() from the R package MatchIt. Please note that this will give us slightly different results than those obtained by Schafer and Kang (2008).\nTo run the matching function, we plug in the same expression as we used above to obtain the propensity scores, and use method = \"nearest\":\n\n# Load the package\nlibrary(MatchIt)\n\n# Run matching model\nmod_matched &lt;- matchit(\n  formula = DIET ~ DISTR.1 + as.factor(BLACK) + as.factor(NBHISP) + GRADE + \n    SLFHLTH + SLFWGHT + WORKHARD + GOODQUAL + PHYSFIT + PROUD + LIKESLF + \n    ACCEPTED + FEELLOVD, \n  method = \"nearest\",  \n  data = df\n)\n\nmod_matched\n\n\n\n\n\n\n\nDescribe the information that is included in the output.\n\n\n\n\n\nThe results indicate that the original sample consisted of 6000 individuals, and that the matched sample consists of 2440 individuals. This is because the number of treated individuals is 1220, and these were all matched with a non-treated person.\nNote that a different model for the propensity scores (e.g., including interaction terms between two covariates, or non-linear relations by squaring covariates), would lead to different propensity scores, and these may subsequently lead to different matches. Hence, it is really model dependent!\n\n\n\nThere are two useful plotting options regarding the propensity scores of our matched pairs: plot(mod_matched, type = \"jitter\") and plot(mod_matched, type = \"hist\").\n\n\n\n\n\n\nGet both plots, and describe what they represent.\n\n\n\n\n\nThe plot(mod_matched, type = \"jitter\") is a plot of the propensity scores of four different subgroups from our original data file:\n\nThose from the treatment group for whom there was no match.\nThose from the treatment group for whom there was a match.\nThose from the control group for whom there was a match.\nThose from the control group for whom there was no match.\n\nIt shows that the first group is empty, the last group has relatively low propensity scores, the middle two groups seem pretty similar in terms of their propensity score distribution (as expected, as these are the matched cases).\nThe plot(mod_matched, type = \"hist\") plot shows the histograms of the propensity scores of the two treatment groups in the original dataset on the left and for the matched groups on the right; it shows that the latter are far more similar than the former (as one would expect).\n\n\n\nTo do the analysis on the matched cases only, we need to create a new data file with only the matched cases, using df_matched &lt;- match.data(mod_matched).\n\n\n\n\n\n\nCreate the Table 1 for this matched data set. What can you conclude?\n\n\n\n\n\n\n# Extract new data file from original one\ndf_matched &lt;- match.data(mod_matched)\n\n# Compute SMDs using tableone R package\ntable1 &lt;- CreateTableOne(\n  vars = c(\"DISTR.1\",\"BLACK\", \"NBHISP\", \"GRADE\", \"SLFHLTH\", \"SLFWGHT\", \n           \"WORKHARD\", \"GOODQUAL\", \"PHYSFIT\", \"PROUD\", \"LIKESLF\", \"ACCEPTED\", \n           \"FEELLOVD\"), \n  strata = \"DIET\", \n  data = df_matched,\n  test = FALSE\n)\nprint(table1, smd = TRUE)\n\nNote that the two groups now each have 1220 cases (compared to 4789 and 1220 respectively before). This is because we are now working with only matched cases, and there was a match in the non-treated group for every treated person.\nThe table shows that the standardized mean differences are all quite small in this matched data set; this means the two groups are now very similar on the covariates, just as one would expect in an RCT. Hence, matching seems to mimic an RCT here—at least with respect to observed covariates; there may still be unobserved confounding.\n\n\n\n\n\n\n\n\n\nInvestigate with a t-test whether the means on the outcome variable DISTR.2 differ among the matched cases.\n\n\n\n\n\n\nmod_t_matched &lt;- t.test(DISTR.2 ~ DIET, df_matched)\nmod_t_matched \n\nThe ACE is now estimated to be \\(-0.0222\\) and not significantly different from 0, \\(t(2437.9) = 1.1663\\), \\(p = .244\\).\n\n\n\n\n\n\n\n\n\nCompare this result to the mean comparison you did at the start. Explain why the mean differences that you have just determined is an estimate of the ACE\\(_{1}\\) rather than of the ACE.\n\n\n\n\n\nInitially, the difference was 0.0596, meaning that those who diet (\\(X=1\\)) experience more distress than those who do not diet (\\(X=0\\)). Here the mean difference between the matched cases is \\(0.703-0.725=-0.022\\), meaning that the distress for those who did diet (\\(X=1\\)) is actually lower than that of those who did not diet (\\(X=0\\)).\nNote that the matched cases are based on all the girls in our initial sample with \\(X=1\\); hence, we now have the ACE for the treated. This implies that for the subpopulation of dieting girls (\\(X=1\\)), actually dieting (\\(X=1\\)) seems to result in less distress than not dieting (\\(X=0\\)). However, the difference is not a significant difference."
  },
  {
    "objectID": "week4_lab.html#method-5-inverse-probability-weighting",
    "href": "week4_lab.html#method-5-inverse-probability-weighting",
    "title": "Lab Exercises",
    "section": "Method 5: Inverse Probability Weighting",
    "text": "Method 5: Inverse Probability Weighting\nWe can also use inverse probability weighting (IPW). In this case, the estimated propensity scores \\(\\pi_{i}\\) are used to determine the probability that an individual \\(i\\) would have received the treatment that they received:\n\nFor the treated (\\(X_{i} = 1\\)), this is simply \\(\\hat{\\pi}_{i}\\).\nFor the non-treated (\\(X_{i} = 0\\)) this is \\(1 − \\hat{\\pi}_{i}\\).\n\nWe can use these probabilities to create weights. The weight are computed by taking the inverse of the propensities (hence, the name). That way, a case that received a treatment that they was very likely to receive, will get a small weight, while a case that received a treatment that they was very unlikely to receive, will get a large weight. Thus, the inverse probability weight indicates the number of persons from the population that this person represents:\n\nFor the treated, this weight is \\(\\frac{1}{\\hat{\\pi}_{i}\\).\nFor the untreated, it is \\(\\frac{1}{1 - \\hat{\\pi}_{i}\\)\n\n\n\n\n\n\n\nCompute the ACE using IPW. More specifically, use the propensity scores that we estimated earlier, and Equation 20 of Schafer and Kang (2008).\n\n\n\n\n\n\n# Select the exposure and outcome variables\nY &lt;- df$DISTR.2\nX &lt;- df$DIET\n\n# Compute the expected potential outcome under X = 1\nmu1hat &lt;- sum( X*Y/ps ) / sum(X/ps)\n\n# Compute the expected potential outcome under X = 0\nmu0hat &lt;- sum( (1-X)*Y/(1-ps) ) / sum((1-X)/(1-ps))\n\n# Take the difference to get ACE\nmu1hat - mu0hat\n\nThe latter difference is the estimate of the ACE. Obtaining a p-value for this, is tricky, because the p-values need to correct reflect the uncertainty in both the estimation of the propensity scores, as well as the uncertainty in the estimation of the expected potential outcomes.\nYou might wonder how this process works when we don’t have a binary outcome, but a continuous one. In the lecture on Marginal Structural Models, we will learn this. For now, it is important that you develop a little bit of an intuition as to why inverse probability weighting works."
  },
  {
    "objectID": "week4_lab.html#method-6-subclassification",
    "href": "week4_lab.html#method-6-subclassification",
    "title": "Lab Exercises",
    "section": "Method 6: Subclassification",
    "text": "Method 6: Subclassification\nSubclassification, also known as stratification and closely related to the idea of standardization in the causal inference literature, is a method that consists of creating classes (strata) based on the propensity scores. The idea is that the individuals within each stratum are rather similar with respect to their propensity score, and thus with respect to the entire set of covariates on which the propensity score is based. If the covariates are well-balanced within each stratum, this is a way to mimic an RCT within each stratum. By subsequently estimating the ACE in each stratum (using a mean comparison such as Method 1, or an ANCOVA or regression analysis such as Method 2), we can determine the causal effect for individuals who are similar with regard to the entire set of covariates (as these are used to determine the propensity scores).\n\n\n\n\n\n\nBegin with creating five strata based on the propensity scores (for instance, use the function cut() in R). Each stratum should contain 20% of the (total number of) observations.\n\n\n\n\n\nThe below R code divides the range of the propensity scores into intervals, and codes these intervals accordingly.\n\ndf$stratum &lt;- cut(\n  x = df$ps, \n  breaks = c(quantile(df$ps, probs = seq(0, 1, 0.2))),\n  labels = seq(1:5),\n  include.lowest = TRUE\n)\n\nWe can also make a plot of these quantiles. This is based on using the same histogram of the propenisty scores we had before, but now adding vertical lines for where the breaks of the strata are.\n\n# Print overlapping plots\nplot(hist0, \n  col = rgb(0, 0, 1, 1/4), \n  xlim = c(0, 1),\n  xlab = \"Propensity score\", \n  main = \"Histogram of propensity scores \\nwith quantile breaks\"\n)  \nplot(hist1, \n  col = rgb(1, 0, 0, 1/4), \n  xlim = c(0, 1), \n  add = TRUE\n) \n\n# Save the location of the break points\nbr &lt;- c(quantile(df$ps, probs = seq(0, 1, 0.2)))\n\n# Plot the borders of the strata\nabline(v = br[2], col = \"black\", lwd = 3)\nabline(v = br[3], col = \"black\", lwd = 3)\nabline(v = br[4], col = \"black\", lwd = 3)\nabline(v = br[5], col = \"black\", lwd = 3)\n\nThis shows that especially the fifth stratum is very wide. In fact, Schafer and Kang (2008) decide to further split the fourth stratum in two groups, and the fifth in four groups, because these are rather wide. The problem with these wide strata is that you cannot reasonably say that the people who belong to these strata are similar to each other on the covariates.\nWe could also further investigate whether we need more strata by looking at the standardized mean differences in each stratum. These should be small, as the idea is that each stratum can be thought of as an RCT in which the assignment to treatment is random, and thus does not depend on any of the covariates.\n\n\n\n\n\n\n\n\n\nNext, compute the ACE in each stratum based on the mean difference.\n\n\n\n\n\nThe below R code performs a t-test in each stratum.\n\n# Initialize matrix to save results in\nresults &lt;- matrix(NA, 5, 1)\n\nfor (quintiles in c(1:5)) {\n  fit &lt;- t.test(DISTR.2 ~ DIET, data = df[which(df$stratum == quintiles), ])\n  print(fit)\n  \n  # Save difference in means\n  results[quintiles, 1] &lt;- fit$estimate[2] - fit$estimate[1]\n}\n\nThis shows that the results differ per stratum: Only in stratum 4 do we find a significant difference.\n\n\n\n\n\n\n\n\n\nSubsequently, you can compute the overall ACE by taking the average of the stratum-specific ACE’s (weighted by the stratum size).\n\n\n\n\n\nSince our five strata are based on quantiles, the sample size of each stratum will be the same (i.e., a fifth of the total sample size). such that each stratum-specific ACE adds equally to the total. Note that this also means that our ACE estimate will differ somewhat from the ACE estimate reported in Table 6 by Schafer and Kang, as they had further divided the fifth stratum.\nTo get the ACE, we simply take the mean of the stratum-specific ACEs:\n\nmean(results[, 1])\n\nNote that we do not have an SE (nor a p-value) for this estimate. Like with Method 5, this is a bit more complicated to obtain."
  },
  {
    "objectID": "week4_lab.html#conclusion",
    "href": "week4_lab.html#conclusion",
    "title": "Lab Exercises",
    "section": "Conclusion",
    "text": "Conclusion\nThe three methods that we considered in this lab are all based on using the propensity score, that is, the probability of being treated given the covariates. The goal of these techniques is to somehow mimic the situation we get in an RCT, where the probability of treatment is independent of the covariates. In matching, this is done by creating pairs of a treated and an untreated person who have (almost) identical propensity scores, resulting in a smaller but balanced dataset; in inverse probability weighting, this is done by weighing each person’s observation by their inverse probability of received treatment, thereby creating a balanced pseudo-population; and in subclassification this is done by creating strata based on the propensity scores such that within each stratum the covariates are balanced. In each approach, we should check whether it balances the covariates, for instance, by considering the standardized mean differences. Other options for assessing covariate imbalance are described by Austin (2009)."
  },
  {
    "objectID": "week3_lab_noAnswers.html",
    "href": "week3_lab_noAnswers.html",
    "title": "Lab Exercises",
    "section": "",
    "text": "In these exercises, we follow the analyses discussed in Schafer and Kang (2008), focusing on Methods 1 to 3. We will also use the data from Schafer and Kang (2008), which are in a data file called SchaferKangData.dat. These are simulated data, and hence the authors (and we) know what the correct answer to the question “What is the effect of dieting on emotional distress?” actually is. Hence, the purpose of the exercises here is to obtain a deeper understanding and hands-on experience with the diverse techniques.\nMake sure to compare the results you get throughout the exercises to those reported in Table 6 of Schafer and Kang (2008). For null-hypothesis tests, you can use a significance level of 0.05 throughout."
  },
  {
    "objectID": "week3_lab_noAnswers.html#setup",
    "href": "week3_lab_noAnswers.html#setup",
    "title": "Lab Exercises",
    "section": "Setup",
    "text": "Setup\nIn this practical you will make use of various R packages. If you haven’t already, install the packages tableone using install.packages(\"tableone\").\nLoad the data, which are in the data file called SchaferKangData.dat. Take a look at the data set. See Table 3 in Schafer and Kang (2008) for a description of the variables.\n\ndf &lt;- read.table(\"SchaferKangData.dat\", header = TRUE)\nhead(df, n = 10)"
  },
  {
    "objectID": "week3_lab_noAnswers.html#method-1-compare-means",
    "href": "week3_lab_noAnswers.html#method-1-compare-means",
    "title": "Lab Exercises",
    "section": "Method 1: Compare Means",
    "text": "Method 1: Compare Means\nWhen a randomized controlled trial (RCT) has been conducted, the treatment groups should not differ on any (pre-treatment) covariate due to random assignment. In that case, the ACE can be computed by taking the difference in means between the two groups for the outcome variable. This is also referred to as the prima facie effect. Although the current data are not generated by a RCT scenario, we will nevertheless consider this naive estimation approach for the causal effect.\n\n\n\n\n\n\nCompare the means between the groups on the outcome variable. Use, for instance t.test() in R."
  },
  {
    "objectID": "week3_lab_noAnswers.html#investigating-covariate-imbalance",
    "href": "week3_lab_noAnswers.html#investigating-covariate-imbalance",
    "title": "Lab Exercises",
    "section": "Investigating Covariate Imbalance",
    "text": "Investigating Covariate Imbalance\nThe mean difference above is based on the assumption that there is no confounding. However, we have a set of observed covariates, and if our data mimic an RCT, there should be no mean differences between the two groups on these covariates. We check this with the standardized mean difference (rather than a t-test, because we do not want it to be dependent on sample size), that is \\[\\Delta Z = \\frac{(\\bar{Z} | X = 1) - (\\bar{Z} | X = 0)}{\\sqrt{((S^{2} | X = 1) +(S^{2} | X = 0)) / 2}},\\] where \\(\\bar{Z} | X = x\\) is the mean of covariate \\(Z\\) in group \\(x\\), \\(S^{2} | X = x\\) is the variance in group \\(x\\), and \\(X = 1\\) is the diet group and \\(X = 0\\) is the non-diet group.\n\n\n\n\n\n\nDetermine the normalized difference between the girls who did diet versus the girls who did not diet on the first covariate, that is, DISTR.1 (emotional distress at wave 1).\n\n\n\n\n\n\n\n\n\nInstead of computing these SMDs ourselves, we can also use the function CreateTableOne() from the R package tableone:\n\nlibrary(tableone) # Load the package\n\ntable1 &lt;- CreateTableOne( # Create SMDs for all covariates\n  vars = c(\"DISTR.1\",\"BLACK\", \"NBHISP\", \"GRADE\", \"SLFHLTH\", \"SLFWGHT\", \n           \"WORKHARD\", \"GOODQUAL\", \"PHYSFIT\", \"PROUD\", \"LIKESLF\", \"ACCEPTED\",\n           \"FEELLOVD\"), \n  strata = \"DIET\", \n  data = df,\n  test = FALSE\n)\n\n\n\n\n\n\n\nObtain the table with standardize mean differences using print(table1, smd = TRUE), and comment on wheter there is imbalance on the covariates across the groups."
  },
  {
    "objectID": "week3_lab_noAnswers.html#method-2-ancova",
    "href": "week3_lab_noAnswers.html#method-2-ancova",
    "title": "Lab Exercises",
    "section": "Method 2: ANCOVA",
    "text": "Method 2: ANCOVA\nMethod 2 of Schafer and Kang (2008) is based on including possible confounders as covariates (or predictors) in a model for the outcome: When there are no interactions between predictors in the model they refer to the analysis as ANCOVA, with interactions between predictors it is more generally referred to as “regression”. We will include all the covariates in our analyses below, even when they have a small standardized mean difference. In the latter case there may not be a need to correct for differences in them between the groups, but they may still account for variance in the outcome variable, and by accounting for this, we increase the power of our analysis.\nTo run an ANCOVA, we can simply run a regression model without interactions between the predictors. Our model here can be written as: \\[\n\\begin{align}DISTR.2 = &\\alpha + \\theta DIET + \\\\\n&\\beta_{1} DISTR.1 + \\beta_{2} BLACK + \\beta_{3} NBHISP + \\beta_{4} GRADE + \\beta_{5} SLFHLTH + \\\\\n&\\beta _{6} SLFWGHT + \\beta_{7} WORKSHARD + \\beta_{8} GOODQUAL + \\beta_{9} PHYSFIT + \\\\\n&\\beta_{10} PROUD + \\beta_{11} LIKESLF + \\beta_{12} ACCEPTED + \\beta_{13} FEELLOVED\n\\end{align}\n\\] Note, since the categorical variables \\(DIET\\), \\(BLACK\\) and \\(NBHISP\\) are dummy variables (they only have 2 categories), you do not need to treat them differently than the continuous covariates.\n\n\n\n\n\n\nRun the ANCOVA model (for instance using the function glm() in R), and interpret the results.\n\n\n\n\n\n\n\n\n\nWe can extend the ANCOVA model above by incorporating product terms between the predictors:\n\nproduct between covariate and itself: quadratic effect\nproduct between two covariates: interaction\nproduct between treatment and covariates: non-parallel planes, see Figure 2 in Schafer and Kang (2008).\n\nWe will not proceed with this here—note that Schafer and Kang (2008) consider the latter option in their paper—but two additional comments are in place:\n\nAdding such product terms quickly increases the number of parameters that we need to estimate; this reduces the power (especially relevant when dealing with small sample sizes).\nBefore including interactions, one needs to center the covariates, and the interaction terms then needs to be centered again; failing to do so implies the main effect for treatment may not represent the average causal effect of treatment."
  },
  {
    "objectID": "week3_lab_noAnswers.html#method-3-regression-estimation",
    "href": "week3_lab_noAnswers.html#method-3-regression-estimation",
    "title": "Lab Exercises",
    "section": "Method 3: Regression Estimation",
    "text": "Method 3: Regression Estimation\nRegression estimation is not the same as regression analysis (which was discussed above). In regression estimation, we make actual predictions of the potential outcomes that were not observed, and use these to compute the causal effect of interest.\nTo use regression estimation, you have to:\n\nDivide the data set into those who were treated and those who were not treated.\nEstimate a regression model (with all the covariates) in each group separately.\nObtain the parameter estimates from each group; see Equations (13) and (14) in Schafer and Kang (2008).\nUse these, and the covariates to predict the potential outcomes \\(\\hat{Y}^{0}\\) and \\(\\hat{Y}^1\\) for each person.\nCompute the average difference between these predicted potential outcomes.\n\nHence, this approach is a technique to impute the missing values in the data file that only contains the potential outcomes that were observed. As this is a somewhat more challenging approach, some of the code will be presented immediately, so that you can work from there to answer the questions.\nWe start with creating separate data sets for the two treatment groups, and running a regression analysis for each group separately:\n\n# Subset the data\ndf1 &lt;- subset(df, DIET==1) # Diet group\ndf0 &lt;- subset(df, DIET==0) # Non-diet group\n\n# Regression analysis with only people with X = 1\nmod_diet &lt;- glm(\n  formula = DISTR.2 ~ + DISTR.1 + as.factor(BLACK) + as.factor(NBHISP) + GRADE + \n    SLFHLTH + SLFWGHT + WORKHARD + GOODQUAL + PHYSFIT + PROUD + LIKESLF + \n    ACCEPTED + FEELLOVD,\n  data = df1\n)\n\n# Regression analysis with only people with X = 0\nmod_nondiet &lt;- glm(\n  formula = DISTR.2 ~ + DISTR.1 + as.factor(BLACK) + as.factor(NBHISP) + GRADE + \n    SLFHLTH + SLFWGHT + WORKHARD + GOODQUAL + PHYSFIT + PROUD + LIKESLF + \n    ACCEPTED + FEELLOVD,\n  data = df0\n)\n\nNow we can obtain estimates for everyone (whether we observed \\(X=1\\) or \\(X=0\\)) for the potential outcome under treatment \\(Y^1\\) and under no treatment \\(Y^1\\):\n\n# Predict outcome for all cases using parameter estimates based on only diet group\npreds_Y1 &lt;- predict(mod_diet, newdata = df)\n\n# Predict outcome for all cases using parameter estimates based on only non-diet group\npreds_Y0 &lt;- predict(mod_nondiet, newdata = df)\n\n\n\n\n\n\n\nHow can the results be used to estimate the ACE?\n\n\n\n\n\n\n\n\n\nAbove we have used the predicted potential outcomes for everyone. However, one of the potential outcomes is actually observed, and we could use that one instead of the predicted potential outcomes (i.e., we use the observed fact, and predict only the counterfact). Hence, for individuals in the no treatment condition you use the observed outcome \\(Y\\) rather than predicted the predicted counterfact \\(\\hat{Y}^0\\), and for those in the treatment condition you use the observed \\(Y\\) instead of the predicted counterfact \\(\\hat{Y}^1\\).\n\npreds_Y0_onlyCounterfact &lt;- preds_Y0 # Copy predicted potential outcomes\npreds_Y0_onlyCounterfact[df$DIET==0] &lt;- df$DISTR.2[df$DIET==0] # Replace observed potential outcomes with factual\n\npreds_Y1_onlyCounterfact &lt;- preds_Y1 # Copy predicted potential outcomes\npreds_Y1_onlyCounterfact[df$DIET==1] &lt;- df$DISTR.2[df$DIET==1] # Replace observed potential outcomes with factual\n\n\n\n\n\n\n\nCheck whether this leads to a different result."
  },
  {
    "objectID": "week3_lab_noAnswers.html#conclusion",
    "href": "week3_lab_noAnswers.html#conclusion",
    "title": "Lab Exercises",
    "section": "Conclusion",
    "text": "Conclusion\nIn both Method 2 and 3 we have assumed linear relations between the covariates \\(Z\\) and the outcome \\(Y\\). This is a parametric assumption that we make in Phase III additional to the causal identification assumptions made in Phase II."
  },
  {
    "objectID": "week2_lab.html",
    "href": "week2_lab.html",
    "title": "Lab Exercises",
    "section": "",
    "text": "In these exercises, we practice with d-separation by reasoning about them, and through the use of simulations. When using simulations, we use a software program (in this case R) to generate data ourselves, such that we know exactly what the correct relationships are between the \\(X\\) variable and the \\(Y\\) that we created. We can then use fit different models with different adjustment sets to the simulated data, and check if we get the correct answer out.\nAfter these exercises, you should be able to:",
    "crumbs": [
      "Week 2: Directed Acyclical Graphs",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week2_lab.html#substance-abuse",
    "href": "week2_lab.html#substance-abuse",
    "title": "Lab Exercises",
    "section": "Substance Abuse",
    "text": "Substance Abuse\nSuppose we are interested in substance abuse among adolescents. After talking to several experts, we have come up with the below DAG.\n\n\n\nDAG of adolescent substance abuse.\n\n\n\n\n\n\n\n\nWrite down all the paths that connect parental alcohol use (\\(PAU\\)) to adolescent substance abuse (\\(ASA\\)).\n\n\n\n\n\nThe following paths connect \\(PAU\\) and \\(ASA\\):\n\n\\(PAU \\rightarrow ASA\\) (direct causal effect)\n\\(PAU \\rightarrow FC \\rightarrow ASA\\) (indirect causal effect)\n\\(PAU \\rightarrow FC \\rightarrow AD \\rightarrow ASA\\) (indirect causal effect)\n\\(PAU \\rightarrow AD \\rightarrow ASA\\) (indirect causal effect)\n\\(PAU \\rightarrow AD \\leftarrow FC \\rightarrow ASA\\) (noncausal path, blocked by \\(AD\\))\n\n\n\n\n\n\n\n\n\n\nWould it help or harm to control for family conflict (\\(FC\\))?\n\n\n\n\n\nThis would be harmful, as it would block the causal paths \\(PAU \\rightarrow FC \\rightarrow ASA\\) (indirect causal effect) and \\(PAU \\rightarrow FC \\rightarrow AD \\rightarrow ASA\\) (indirect causal effect). We would thus create over-control bias due to conditioning on a mediator.\n\n\n\n\n\n\n\n\n\nWould it help or harm to control for adolescent depression (\\(AD\\))?\n\n\n\n\n\nThis would be harmful, as it would block the following causal paths:\n\n\\(PAU \\rightarrow AD \\rightarrow ASA\\) (indirect causal effect)\n\\(PAU \\rightarrow FC \\rightarrow AD \\rightarrow ASA\\) (indirect causal effect)\n\nFurthermore, it would open this currently blocked non-causal path:\n\n\\(PAU \\rightarrow AD \\leftarrow FC \\rightarrow ASA\\) (noncausal path, blocked by \\(AD\\))\n\nWe would thus introduce two forms of bias: Over-control bias due to conditioning on a mediator, and collider bias due to conditioning on a collider.\n\n\n\n\n\n\n\n\n\nSuppose that we are interested in the causal effect of adolescent depression (\\(AD\\)) on adolescent substance abuse (\\(ASA\\)). Based on the DAG above, should we or should we not control for parental alcohol use (\\(PAU\\)) and/or family conflict (\\(FC\\))?\n\n\n\n\n\nFirst we need to write down the paths that connect \\(AD\\) to \\(ASA\\):\n\n\\(AD \\rightarrow ASA\\) (causal path)\n\\(AD \\leftarrow FC \\rightarrow ASA\\) (open back-door)\n\\(AD &lt;- FC &lt;- PAU -&gt; ASA\\) (open back-door)\n\\(AD &lt;- PAU -&gt; FC -&gt; ASA\\) (open back-door)\n\\(AD &lt;- PAU -&gt; ASA\\) (open back-door)\n\nTo close all four open back-door paths, we should condition on both \\(PAU\\) and \\(FC\\). Neither of these is a collider on any of the paths, so there is no harm when conditioning on them.",
    "crumbs": [
      "Week 2: Directed Acyclical Graphs",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week2_lab.html#discrimination",
    "href": "week2_lab.html#discrimination",
    "title": "Lab Exercises",
    "section": "Discrimination",
    "text": "Discrimination\nLet’s consider a real-world example from the online book The Mixtape by Scott Cunningham. It is based on the question whether there is gender discrimination in labor markets. Specifically, it focuses on the criticism that Google would underpay female employees, by showing a gender pay gap. Google responded by saying that the difference between males and females in pay disappear once differences in occupation are accounted for.\nCunningham raises the following question:\n\n“But what if one of the ways gender discrimination creates gender disparities in earnings is through occupational sorting? If discrimination happens via the occupational match, than naïve contrasts of wages by gender controlling for occupational characteristics will likely understate the presence of discrimination in the marketplace.”\n\nCunningham also presents a DAG for this problem, which is displayed below.\n\n\n\nA DAG of gender discrimination regarding earnings.\n\n\n\n\n\n\n\n\nWhat is the causal effect of discrimination (\\(D\\)) on earning (\\(E\\))?\n\n\n\n\n\nThere are two causal paths:\n\n\\(D \\rightarrow E\\) (direct causal path)\n\\(D \\rightarrow O \\rightarrow E\\) (indirect causal path)\n\n\n\n\n\n\n\n\n\n\nWhat happens when we condition on occupation (\\(O\\)), like Google did in their reply? Does it provide a less or a more biased estimate of the effect of discrimination on earnings?\n\n\n\n\n\nConditioning on \\(O\\) blocks the indirect causal path \\(D \\rightarrow O \\rightarrow E\\). Hence, it helps to get us the direct effect of discrimination, which does not operate through occupation. This is perhaps in line with what some may consider the right comparison: Correcting for occupation is a way to see whether men and women with the same job receive the same salary or not. However, according to this DAG, there may also be an indirect effect of discrimination (i.e., women are hired into other kinds of jobs than men; this could be a case of discrimination in the company, or perhaps this selection process already occurs during one’s education). By focusing only on the direct effect of discrimination on salary, we can see whether the company should raise the salary of women to create (more, but not necessarily total) fairness across the sexes.\nBut there is another problem that arises here. Conditioning on occupation actually opens the path \\(D \\rightarrow O \\leftarrow A \\rightarrow E\\) which will introduces collider bias. While abilities and being female (and discrimination) are independent of each other, by conditioning on occupation, they become dependent, and it thus leads to a non-causal association between discrimination and earnings.\nHence, conditioning on occupation is not giving us the right answer, even if we are sure we are only interested in the direct effect of discrimination on earnings (and not on indirect effects operating through occupation).\nThe only solution then is to make sure we also get a measure of ability, such that we can close this opened backdoor path and thereby avoid the collider bias that arises when conditioning on occupation.",
    "crumbs": [
      "Week 2: Directed Acyclical Graphs",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week2_lab.html#conclusion",
    "href": "week2_lab.html#conclusion",
    "title": "Lab Exercises",
    "section": "Conclusion",
    "text": "Conclusion\nThe reasoning exercises above have illustrated the use of DAGs and the back-door criterion for deciding which covariates to condition on, to identify a particular causal effect. A few highlights of this are:\n\nThere may be multiple sets of covariates (known as adjustment sets) that one can condition on to block open backdoor paths. We can use this kind of information when designing a study, to decide what variables to measure. It may be the case that one set is smaller than another, but that it contains a variable that is hard or impossible to measure; in that case we may choose to focus on a different, larger set, because that is more practical in the end.\nPerhaps needless to say: The set of covariates one should condition on depends on which causal effect you are looking at. So you have to be clear what you are focusing on in you DAG.\nAs the possible gender discrimination example illustrates, DAGs can be very useful in organizing an communicating one’s ideas about reality.",
    "crumbs": [
      "Week 2: Directed Acyclical Graphs",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week2_lab.html#exam-scores",
    "href": "week2_lab.html#exam-scores",
    "title": "Lab Exercises",
    "section": "Exam Scores",
    "text": "Exam Scores\nThis exercises is based on an example from Ben Prytherch. It concerns the score a student receives for their exam and which factors determine this. Prytherch presents the following DAG, which shows that the score depends on both the difficulty of the exam, and the time a student studied for the exam. Moreover, the time a student studied also depends on the difficulty of the exam.\n\n\n\nA DAG of the effect of time studying on exam scores.\n\n\nUse the R-code below to simulate data.\n\nset.seed(285)\nN &lt;- 1000 # Sample size\nD &lt;- rnorm(N) # Randomly generate the difficulty variable from a normal distribution\nTS &lt;- 44 + 0.7*D + rnorm(N,0,1) # Create the `time_studying` variable \nS &lt;- 100 - 0.8*D + 0.4*TS + rnorm(N,0,1) # Create the `score` variable\n\ndat &lt;- data.frame(D = D, TS = TS, S = S)\n\n\n\n\n\n\n\nIs there anything you notice in particular when inspecting the correlations?\n\n\n\n\n\nInspecting the correlations between the variables using round(cor(dat), 2) shows that the correlation between Time_Studying and Score is very close to zero. In a super naive interpretation one would conclude that it does not make a difference whether one studies a lot or not. However, this is of course not the full story.\n\n\n\n\n\n\n\n\n\nSuppose we are interested in the effect of Time_Studying on Score. Should we control for Difficulty? Why (not)?\n\n\n\n\n\nYes we should, because Difficulty is a confounder here.\n\n\n\n\n\n\n\n\n\nRun a regression analysis for estimating the effect of Time_Studying on Score, and think yourself about whether you should additionally control for Difficult.\n\n\n\n\n\n\nfit &lt;- lm(S ~ D + TS, data = dat) # Run regression analysis\nsummary(fit) # Inspect the results\n\nWhen accounting for the confounder Difficulty, the effect of Time_Studying on Score is \\(0.403\\), which is close to the value \\(0.4\\) with which these data were generated.\n\n\n\n\n\n\n\n\n\nAssuming you just ran a model that included both Time_Studying and Difficulty as predictors of Score, now run a model that only includes Time_Studying. How do you interpret the results of this analysis?\n\n\n\n\n\n\nfit &lt;- lm(S ~ TS) # Run regression analysis\nsummary(fit) # Inspect the results\n\nThe current parameter is a biased estimate of the causal effect of Time_Studying on Score; the bias is due to the confounder Difficulty, which has a positive effect on Time_Studying, but a negative effect on Score. As a result, this bias is negative, and it obscures the positive causal effect of Time_Studying on Score (which we were able to correctly recover in the previous analysis).\n\n\n\n\n\n\n\n\n\nSuppose we are interested in the effect of Difficulty on Score. What happens if we condition on Time_Studying?\n\n\n\n\n\nTime studying is a mediator here. If we condition on it, this blocks the indirect path from Difficultyto Score, such that we only get the direct effect of Difficulty on Score. If we do not condition on Time_Studying, we get the total effect of Difficulty on Score, so the sum of the direct and the indirect (mediated) effects.\n\n\n\n\n\n\n\n\n\nAssuming you just ran a model that included both Time_Studying and Difficulty as predictors of Score, now run a model that only includes Time_Studying. How do you interpret the results of this?\n\n\n\n\n\nIn the first model, we get the total effect of Difficulty on Score. It is estimated to be \\(-0.520\\), meaning that more higher difficulty leads to lower scores. Note however that this estimate is closer to zero than the actual direct effect with which we simulated the data, which was \\(-0.8\\).\nWhen considering the results from the second model, we see that when we condition on Time_Studying, the estimate of the direct causal effect of Difficultyon Score is \\(-0.811\\), which is quite close to the underlying Truth.\nIn this case, we thus see that Difficultyhas a negative effect on Score, but this is to some extent countered by the fact that Difficulty leads to more Time_Studying, which in turn increases Score.\n\n\n\n\n\n\n\n\n\nSuppose that we are interested in the effect of Difficulty on Time_Studying. What happens if we condition on Score?\n\n\n\n\n\nScore is a child of both Difficulty and Time_Studying. Hence, conditioning on this will result in collider bias.\n\n\n\nTo illustrate the problem that can arise when conditioning on a collider, create a new dataset using the R code below. This time, Difficulty has no effect on Time_Studying (i.e., the effect is zero) in the data generating model.\n\nset.seed(285)\nN &lt;- 1000\nD &lt;- rnorm(N)\nTS &lt;- 44 + 0*D + rnorm(N,0,1)\nS &lt;- 100 - 0.8*D + 0.4*TS + rnorm(N,0,1)\n\ndat &lt;- data.frame(D = D, TS = TS, S = S)\n\n\n\n\n\n\n\nNow run a model to estimate the effect of Difficulty on Time_Studying, once without conditioning on Score, and once while conditioning on Score. How do you interpret the results?\n\n\n\n\n\nThis result shows that when we just look at the effect of Difficulty on Time_Studying, we find a zero effect (the estimate is \\(0.022\\)). However, if we condition on Score, then the effect of Difficulty on Time_Studying becomes positive; it is then estimated to be \\(0.281\\).\nThe latter result can be understood as follows: Within a group of students who score similar on the exam, if the difficulty was higher for a specific student, this student also tended to study longer, while if the difficulty of the exam was lower for a specific student, this student would study less than the average student in this group.",
    "crumbs": [
      "Week 2: Directed Acyclical Graphs",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week1_lab.html",
    "href": "week1_lab.html",
    "title": "Lab Exercises",
    "section": "",
    "text": "To get more acquainted with potential outcomes and formulating causal questions, please do the potential outcome exercises and the causal estimand exercises of the Center for Practice and Research at the Intersection of Information, Society, and Methodology (PRIISM).\n\nNew York University PRIISM (n.d.). thinkCausal. https://apsta.shinyapps.io/thinkCausal/\n\nIn the left menu, select Learn \\(\\rightarrow\\) Potential outcomes. For the causal estimand exercises, select Learn \\(\\rightarrow\\) Causal estimand.",
    "crumbs": [
      "Week 1: Introduction and Fundamental Concepts",
      "Lab exercises"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Syllabus",
    "section": "",
    "text": "You can find the course schedule in the overview below. Lectures and labs will take place in the Wolfgang-Köhler-Haus (Rudower Chaussee 18), room 2.101 (Testbibliothek).\n\n\n\n\n\n\n\n\nWeek\nWhat\nTopic\n\n\n\n\n43\nReading\nHernán (2018)\n\n\n\nLecture\nIntroduction and fundamental concepts\n\n\n\nLab\nExercises\n\n\n44\nReading\nRohrer (2018)\n\n\n\nLecture\nCausal graphs\n\n\n\nLab\nExercises\n\n\n45\nReading\nSchafer and Kang (2008)\n\n\n\nLecture\nRubin Causal Model (Part I)\n\n\n\nLab\nExercises\n\n\n46\nReading\nSchafer and Kang (2008)\n\n\n\nLecture\nRubin Causal Model (Part II)\n\n\n\nLab\nExercise, start assignment\n\n\n47\nReading\nVanderWeele, Jackson, and Li (2016)\n\n\n\nLecture\nMarginal Structural Models and time-varying exposures\n\n\n\nLab\nExercises\n\n\n48\nReading\nMulder et al. (2024)\n\n\n\nLecture\nEstimating causal effects of time-varying exposures\n\n\n\nLab\nExercises\n\n\n49\nLab\nAssignment: Preparation\n\n\n50\nLecture\nAssignment: Presentations\n\n\n\n\n\n\n\nReferences\n\nHernán, Miguel A. 2018. “The C-Word: Scientific Euphemisms Do Not Improve Causal Inference from Observational Data.” American Journal of Public Health 108 (5): 616–19. https://doi.org/10.2105/AJPH.2018.304337.\n\n\nMulder, Jeroen D., Kim Luijken, Bas B. L. Penning de Vries, and Ellen L. Hamaker. 2024. “Causal Effects of Time-Varying Exposures: A Comparison of Structural Equation Modeling and Marginal Structural Models in Cross-Lagged Panel Research.” Structural Equation Modeling: A Multidisciplinary Journal. https://doi.org/10.1080/10705511.2024.2316586.\n\n\nRohrer, Julia M. 2018. “Thinking Clearly about Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629.\n\n\nSchafer, Joseph L., and Joseph Kang. 2008. “Average Causal Effects from Nonrandomized Studies: A Practical Guide and Simulated Example.” Psychological Methods 13 (4): 279–313. https://doi.org/10.1037/a0014268.\n\n\nVanderWeele, Tyler J., John W. Jackson, and Shanshan Li. 2016. “Causal Inference and Longitudinal Data: A Case Study of Religion and Mental Health.” Social Psychiatry and Psychiatric Epidemiology 51 (11): 1457–66. https://doi.org/10.1007/s00127-016-1281-9.",
    "crumbs": [
      "This Course",
      "Schedule"
    ]
  },
  {
    "objectID": "description.html",
    "href": "description.html",
    "title": "Description",
    "section": "",
    "text": "In psychology and related disciplines, researchers commonly ask causal questions, albeit oftentimes implicitly so. For example, psychologists might be interested in how light physical activity might reduce depressive symptoms, if lower pay in a profession leads to fewer men choosing this profession, or if satisfaction with one’s social contacts affects self-esteem. While a randomized controlled trial (RCT) is considered the golden standard for studying such questions, there are often practical or ethical constraints that prohibit us from doing so, and researchers have to resort to using observational (i.e, nonexperimental) data.\nTo answer causal questions using observational data, we need to be extremely clear about what our causal question is, think carefully about what data we need for identifying it, and make an informed decision about which analysis techniques allows us to draw the most robust conclusions. This course provides you with an introduction to this formal causal research process. You will get acquainted with the basic building blocks of modern causal inference, learn about the steps and assumptions that need to be made throughout a study, and apply various estimation techniques to simulated and empirical (i.e., real-world) observational data. We start with introducing the causal inference building blocks like potential outcomes and directed acyclical graphs (DAGs) from a cross-sectional perspective (i.e., in which all variables have only been measured once). In the second halve of the course, we will apply these building blocks to a longitudinal settings. This allows us to ask new types of interesting causal questions, but also presents new challenges.\nThe causal inference literature provides enough material to fill an entire two-year master program, and we will only scratch the surface of the most important topics therein. Therefore, the goal of the course is (a) provide a motivation for the importance of robust causal inference in much of the modern psychological literature; (b) to introduce you to fundamental concepts of causal inference, and (c) for you to get hands-on experience with the causal research process for observational data. After this course, you can start applying some of these techniques yourself to observational data, and you can critically evaluate (implicitly) causal studies in psychology and related disciplines.",
    "crumbs": [
      "This Course",
      "Description"
    ]
  },
  {
    "objectID": "assignment.html",
    "href": "assignment.html",
    "title": "Assignment",
    "section": "",
    "text": "The purpose of this assignment is for you to demonstrate knowledge of the phases for empirical causal research, and the ability to apply this knowledge to longitudinal, nonexperimental empirical data data. This assignment is done in groups of 3 to 4 students, and should result in a presentation in which you explain your causal inference process, the decisions that were made throughout, and the analysis results. The assignment, grading scheme, and deadlines are explained in more detail below.",
    "crumbs": [
      "Assignment"
    ]
  },
  {
    "objectID": "assignment.html#preparation",
    "href": "assignment.html#preparation",
    "title": "Assignment",
    "section": "Preparation",
    "text": "Preparation\nIn the assignment, you are asked to formulate a causal research question concerning joint effects (Phase I), identify the causal estimand (Phase II), and estimate the effect of interest (Phase III). For phases II and III, you need an empirical dataset. Therefore, you should first find longitudinal, nonexperimental empirical data. These data must include:\n\na time-varying (at least two time points), categorical exposure.\na continuous or binary end-of-study outcome. This outcome can come from a time-varying outcome, from which you select an outcome-measurement after the last exposure-time; or an outcome that was measured only once, but after the last exposure-time (e.g., a distal outcome, or follow-up).\nat least two time-varying covariates (either continuous or categorical).\nat least one baseline covariate (either continuous or categorical).\n\nSome possible sources for empirical data are listed below, but you are free to use any other source for data.\n\nThe Longitudinal Internet Studies for Social Sciences (LISS) panel. Note that for using the LISS data, each student analyzing LISS data has to fill out a LISS statement. Only then you get access to the data. You are not allowed to share data among each other.\nDatadryad. Note that this data finder is more focused on biomedical data.\nGoogle’s data search functionality.\n\nYou are advised to be somewhat “smart”/practical in your choice of dataset. For example, in practice, ideally, you come up with a well-defined research question before you collect data. However, for this assignment you need to make sure that you can actually obtain empirical data that can be used for investigating the research question. Therefore, you are advised to first explore some of the freely available empirical data that exist, before finalizing a causal research question. Furthermore, it might be advisable to find a dataset that includes at least one or two of the covariates that you should adjust for (based on reasoning in Phase II) in your statistical analyses.",
    "crumbs": [
      "Assignment"
    ]
  },
  {
    "objectID": "assignment.html#assignment",
    "href": "assignment.html#assignment",
    "title": "Assignment",
    "section": "Assignment",
    "text": "Assignment\nFor this assignment, go through all the phases of a causal research project. You do this based on a causal research question that you formulate in Phase I, and using the empirical dataset that you selected. In the end, you have to present your progress throughout these phases (and the decisions that you made herein) in a 15-minute presentation (including 2 minutes of questions and answers). Below you can find the steps that you should take to complete the assignment.\n\nPhase I: Formulation\nFormulate a well-defined causal research question concerning joint effects and an end-of-study outcome. Formalize this question as a contrast of potential outcomes, and as a marginal structural model (MSM). You should additionally specify:\n\nThe target population (i.e., what are characteristics of the group of individuals that you want to make inferences about). Also check if your selected empirical data actually contain individuals that are part of your target population.\nWhether your research question concerns an ACE, an ACE\\(_{0}\\), or an ACE\\(_{1}\\).\nThe specific exposure regimes that you want to contrast, including how you want to contrast these regimes (e.g., a risk difference, risk ratio).\nThe outcome, including how, and when this is measured (i.e., the time interval between exposures and the end-of-study outcome);\nHow the MSM relates to your potential outcomes, and which causal assumptions (if any) are embedded in your MSM.\n\nFor assessing the causal identification assumptions in Phase II, it helps if you are very specific here concerning the above points.\n\n\nPhase II: Identification\nAssess the identifiability of the causal estimand by evaluating the causal identification assumptions of sequential conditional exchangeability, sequential consistency, and sequential positivity. For sequential conditional exchangeability:\n\nDraw a causal directed acyclical graph (DAG) to encode the causal assumptions of the causal process under study. This causal DAG should include nodes representing the exposures, the baseline and time-varying covariates, and the end-of-study outcome; and edges to represent the causal relationships between nodes. Discuss which causal assumptions are encoded in the causal DAG.\nBased on the causal DAG, discuss which covariates you need to adjust for.\nEvaluate whether or not you can actually adjust for these covariates using the empirical dataset you selected (i.e., are all covariates that you want to adjust for actually observed?).\n\nFor (sequential) consistency:\n\nDiscuss if you think that the exposure at a particular time point is sufficiently well-defined. For example, is it clear what a (potentially hypothetical) intervention on the exposure entails? Do there exist multiple versions of such an intervention? If so, are these multiple versions of intervention expected to have different effects, and do the empirical data contain information to distinguish between these different versions of interventions?\n\nFor (sequential) positivity:\n\nDiscuss whether or not there is a policy or condition that prevents particular individuals from obtaining the exposure (this would be a violation of structural positivity, in which, even with infinite data, positivity cannot be achieved).\nCheck, using density plots of propensity scores, whether or not there is empirical positivity.\n\nNote that the goal of this phase is not for you to ensure perfect identifiability at all costs. Instead, this phase is meant for researchers to critically reflect on the plausibility of the causal identification assumptions given the causal estimand and the empirical data, and to clearly state the (additional) assumptions under which identification can be achieved.\n\n\nPhase III: Estimation\nNow that you have evaluated the plausibility of the causal identification assumptions, you can attempt to estimate the effect of interest. First, you should prepare your data for analysis. This might involve:\n\nMerging multiple datasets (e.g., if you have separate datasets for separate measurement waves).\nCleaning the dataset: Removing of individuals with impossible values on exposures, the outcome, and covariates; removing of individuals that do not belong to the target population; selection of individuals that all adhere to the same version of treatment (if there exist multiple versions of treatment).\nDealing with missing data. You can decide to do a complete-case analysis (not ideal, but allowed for this assignment), or to use a single-imputation using the mice R-package. The latter option might be advisable when a complete-cases analysis results in too small a sample. Handling of missing data is not the focus of this assignment and course, so please do not spend too much time on this.\n\nNext, create balancing weights for the time-varying exposures:\n\nFirst assess initial imbalance at each exposure-time.\nEstimate propensity scores, and use these for creating a balanced sample by weighting. Discuss (a) which method/model you used to create the propensity scores, (b) which type of inverse probability weights you created, and (c) the quality of these weights.\nAssess imbalance in the balanced sample. If the sample is not balanced yet after weighting, discuss possible reasons why you might not have been able to balance sample based on the propensity scores. Additionally, mention other steps you can take to try and balance the sample, and see if these work.\n\nFinally, estimate the joint effects after weighting, and attempt to answer your research question. Briefly summarize again which assumptions the validity of your answer relies on, and whether or not these assumptions are plausible.",
    "crumbs": [
      "Assignment"
    ]
  },
  {
    "objectID": "assignment.html#presentation",
    "href": "assignment.html#presentation",
    "title": "Assignment",
    "section": "Presentation",
    "text": "Presentation\nThis assignment should result in a presentation in which you (and your group members) present your progress throughout these phases for the causal research question that you have come up with, and the empirical data that you have selected. In particular, focus on the decisions that were made throughout these phases, and critically reflect on the strengths and weaknesses of your approach. The below tables gives an indication of the amount of time that these various parts approximately should take up in the presentation.\n\n\n\nWhat\nTime (min.)\n\n\n\n\nPhase I: Introduce your research question\n2\n\n\n  Target population\n\n\n\n  ACE, ACE\\(_{0}\\), or ACE\\(_{1}\\)?\n\n\n\n  Exposure regimes\n\n\n\n  Outcome\n\n\n\n  MSM\n\n\n\nPhase II: Assess identifiability\n4\n\n\n  Exchangeability\n\n\n\n  Consistency\n\n\n\n  Positivity\n\n\n\nPhase III: Estimate effect of interest\n7\n\n\n  Initial imbalance\n\n\n\n  Estimate propensity scores\n\n\n\n  Post-balancing imbalance\n\n\n\n  Estimate causal effect\n\n\n\nQ&A\n2",
    "crumbs": [
      "Assignment"
    ]
  },
  {
    "objectID": "assignment.html#deadlines",
    "href": "assignment.html#deadlines",
    "title": "Assignment",
    "section": "Deadlines",
    "text": "Deadlines\nEach group presents their assignment during the meeting on Thursday December 12th. All group members should be present, and take part in the actual presentation (unless otherwise agreed with the course coordination). Additionally, each group should send an email (1 per group) to the teacher, containing (a) the slides of the presentation, (b) annotated and executable R code that was used to analyze the data, and (c) names and student numbers of all the group members. The deadline for this email is Thursday December 12th, at 09:00.\nGroup members are expected to work together on the assignment during the lab meetings (where they are able to ask the lab teacher questions), and in between these meetings as well (please arrange meetings for this yourself).",
    "crumbs": [
      "Assignment"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Causal Inference using Observational Data in the Social Sciences",
    "section": "",
    "text": "This is the course website for “Introduction to Causal Inference using Observational Data in the Social Sciences” (winter term 2024/25), which is an optional seminar of the module “CM 6: Special topics in Psychology” of the “Psychology” master program at Humboldt-Universität. Course materials are distributed through this website, and through the accompanying Moodle course.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "week1_literature.html",
    "href": "week1_literature.html",
    "title": "Literature",
    "section": "",
    "text": "Please read the below article. Use the reading aid and reading questions to help you get a better understanding of the main points of the text.\n\nHernán, M. A. (2018). The C-Word: Scientific Euphemisms Do Not Improve Causal Inference From Observational Data. American Journal of Public Health, 180, 616-619. https://doi.org/10.2105/AJPH.2018.304337\n\n\nReading Aid\nMiguel A. Hernán is an epidemiologist, and hence Hernán (2018) is written more for researchers in biomedical fields. Therefore, the paper might contain terms that are unfamiliar to researchers from the social sciences, such as ‘risk ratios’ and ‘target trial’. It does not matter that you do not fully comprehend this jargon. The goal of reading this article is to understand the main points that are made, because these points similarly apply to research in the social sciences. Answering the below reading questions might help:\n\nWhat is the main problem that is discussed in Hernán (2018)?\nWhat common ‘reaction’ to the problem does the author discuss, and why is this reaction problematic according to him?\nIn what parts of a scientific article should a researcher explicitely talk about ‘the C-word’, according to the author?\nWhat are the two main advantages of being more explicit about causality in scientific articles?",
    "crumbs": [
      "Week 1: Introduction and Fundamental Concepts",
      "Literature"
    ]
  },
  {
    "objectID": "week2_literature.html",
    "href": "week2_literature.html",
    "title": "Literature",
    "section": "",
    "text": "Please read the below article. Use the reading aid and reading questions to help you get a better understanding of the main points of the text.\n\nRohrer, J. M. (2018). Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data. Advances in Methods and Practices in Psychological Science, 1(1), 27-42. https://doi.org/10.1177/2515245917745629\n\n\nReading Aid\nJulia M. Rohrer is a personality psychologist at the Wilhelm Wundt Institute for Psychology, with a strong methodology interest in causal inference based on observational data. A large portion of her work focuses on making technical causal inference research accessible to researchers in psychology and related disciplines. In Rohrer (2018), she introduces directed acyclical graphs in a psychological context. The most important point of this reading material is that you understand what DAGs are at their core. How to control for covariates (i.e., the section “How to control for a variable”) is a topic that we explore in more in detail in week 3.\nAnswering the below reading questions might help focus on the most import points for the purpose of this course:\n\nSimilar to Hernán (2018), Rohrer (2018) addresses the problems associated with causal inference from observational data. She describes (and criticizes) three different strategies by researchers from different areas of psychology to cope with the weaknesses of observational data. What are these? Which of these problematic strategies can be addresses by the use of directed acyclical graphs (DAGs)?\nWhat are the two building blocks of DAGs, and what do they represent?\nWhat is a ‘path’?\nIn the section “Confounding: The Bane of Observational Data”, Rohrer describes how DAGs can help researchers to think about confounding. Which phase of causal research does this relate to? Or, more specifically, which assumption does this relate to?\nShould researchers draw up DAGs based on theory or on data?\nCan a DAG only contain observed, or also unobserved variables?\n\n\n\n\n\n\nReferences\n\nRohrer, Julia M. 2018. “Thinking Clearly about Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629.",
    "crumbs": [
      "Week 2: Directed Acyclical Graphs",
      "Literature"
    ]
  },
  {
    "objectID": "week3_lab.html",
    "href": "week3_lab.html",
    "title": "Lab Exercises",
    "section": "",
    "text": "In these exercises, we follow the analyses discussed in Schafer and Kang (2008), focusing on Methods 1 to 3. We will also use the data from Schafer and Kang (2008), which are in a data file called SchaferKangData.dat. These are simulated data, and hence the authors (and we) know what the correct answer to the question “What is the effect of dieting on emotional distress?” actually is. Hence, the purpose of the exercises here is to obtain a deeper understanding and hands-on experience with the diverse techniques.\nMake sure to compare the results you get throughout the exercises to those reported in Table 6 of Schafer and Kang (2008). For null-hypothesis tests, you can use a significance level of 0.05 throughout.",
    "crumbs": [
      "Week 3: Rubin Causal Model (Part I)",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week3_lab.html#setup",
    "href": "week3_lab.html#setup",
    "title": "Lab Exercises",
    "section": "Setup",
    "text": "Setup\nIn this practical you will make use of various R packages. If you haven’t already, install the packages tableone using install.packages(\"tableone\").\nLoad the data, which are in the data file called SchaferKangData.dat. Take a look at the data set. See Table 3 in Schafer and Kang (2008) for a description of the variables.\n\ndf &lt;- read.table(\"SchaferKangData.dat\", header = TRUE)\nhead(df, n = 10)",
    "crumbs": [
      "Week 3: Rubin Causal Model (Part I)",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week3_lab.html#method-1-compare-means",
    "href": "week3_lab.html#method-1-compare-means",
    "title": "Lab Exercises",
    "section": "Method 1: Compare Means",
    "text": "Method 1: Compare Means\nWhen a randomized controlled trial (RCT) has been conducted, the treatment groups should not differ on any (pre-treatment) covariate due to random assignment. In that case, the ACE can be computed by taking the difference in means between the two groups for the outcome variable. This is also referred to as the prima facie effect. Although the current data are not generated by a RCT scenario, we will nevertheless consider this naive estimation approach for the causal effect.\n\n\n\n\n\n\nCompare the means between the groups on the outcome variable. Use, for instance t.test() in R.\n\n\n\n\n\nYou can perform a two-sample t-test using the R code below:\n\nt.test(DISTR.2 ~ DIET, data = df)\n\nThe mean difference is the prima facie estimate of the average causal effect of dieting on distress. The results indicate that the average of the girls who did diet is \\(0.703 - 0.645= 0.059\\) higher than the average of the girls who did not diet, \\(t = -3.886\\), \\(SE = 0.015\\). This difference is statistically significant, \\(p &lt; .001\\).",
    "crumbs": [
      "Week 3: Rubin Causal Model (Part I)",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week3_lab.html#investigating-covariate-imbalance",
    "href": "week3_lab.html#investigating-covariate-imbalance",
    "title": "Lab Exercises",
    "section": "Investigating Covariate Imbalance",
    "text": "Investigating Covariate Imbalance\nThe mean difference above is based on the assumption that there is no confounding. However, we have a set of observed covariates, and if our data mimic an RCT, there should be no mean differences between the two groups on these covariates. We check this with the standardized mean difference (rather than a t-test, because we do not want it to be dependent on sample size), that is \\[\\Delta Z = \\frac{(\\bar{Z} | X = 1) - (\\bar{Z} | X = 0)}{\\sqrt{((S^{2} | X = 1) +(S^{2} | X = 0)) / 2}},\\] where \\(\\bar{Z} | X = x\\) is the mean of covariate \\(Z\\) in group \\(x\\), \\(S^{2} | X = x\\) is the variance in group \\(x\\), and \\(X = 1\\) is the diet group and \\(X = 0\\) is the non-diet group.\n\n\n\n\n\n\nDetermine the normalized difference between the girls who did diet versus the girls who did not diet on the first covariate, that is, DISTR.1 (emotional distress at wave 1).\n\n\n\n\n\nThat can be done using the R code below:\n\ndf1 &lt;- df[ which(df$DIET == 1), ] # Select diet group\ndf0 &lt;- df[ which(df$DIET == 0), ] # Select non-diet group\n\n# Code the formula for SMD in R\n( mean(df1$DISTR.1) - mean(df0$DISTR.1) ) / ( sqrt( (var(df1$DISTR.1) + var(df0$DISTR.1)) / 2))\n\nThe SMD with regards to the covariate DISTR.1 is \\(0.214\\).\n\n\n\nInstead of computing these SMDs ourselves, we can also use the function CreateTableOne() from the R package tableone:\n\nlibrary(tableone) # Load the package\n\ntable1 &lt;- CreateTableOne( # Create SMDs for all covariates\n  vars = c(\"DISTR.1\",\"BLACK\", \"NBHISP\", \"GRADE\", \"SLFHLTH\", \"SLFWGHT\", \n           \"WORKHARD\", \"GOODQUAL\", \"PHYSFIT\", \"PROUD\", \"LIKESLF\", \"ACCEPTED\",\n           \"FEELLOVD\"), \n  strata = \"DIET\", \n  data = df,\n  test = FALSE\n)\n\n\n\n\n\n\n\nObtain the table with standardize mean differences using print(table1, smd = TRUE), and comment on wheter there is imbalance on the covariates across the groups.\n\n\n\n\n\nRunning print(table1, smd = TRUE) gives:\n\n#                       Stratified by DIET\n#                        0           1           SMD   \n#   n                    4780        1220              \n#   DISTR.1 (mean (SD))  0.62 (0.42) 0.71 (0.45)  0.214\n#   BLACK (mean (SD))    0.26 (0.44) 0.17 (0.38)  0.197\n#   NBHISP (mean (SD))   0.15 (0.35) 0.15 (0.36)  0.021\n#   GRADE (mean (SD))    9.16 (1.39) 9.37 (1.34)  0.152\n#   SLFHLTH (mean (SD))  2.20 (0.93) 2.35 (0.91)  0.171\n#   SLFWGHT (mean (SD))  3.19 (0.76) 3.84 (0.70)  0.895\n#   WORKHARD (mean (SD)) 2.14 (0.91) 2.05 (0.85)  0.093\n#   GOODQUAL (mean (SD)) 1.80 (0.67) 1.84 (0.71)  0.055\n#   PHYSFIT (mean (SD))  2.24 (0.93) 2.53 (0.93)  0.320\n#   PROUD (mean (SD))    1.76 (0.77) 1.86 (0.79)  0.126\n#   LIKESLF (mean (SD))  2.09 (0.99) 2.52 (1.06)  0.420\n#   ACCEPTED (mean (SD)) 2.14 (1.00) 2.35 (1.06)  0.207\n#   FEELLOVD (mean (SD)) 1.78 (0.83) 1.93 (0.90)  0.172\n\nMost of the SMDs (the last column) are larger than the rule of thumb of 0.1. This implies that there is considerable imbalance across the two groups with respect to these covariates. This means the data do not mimic an RCT very well. Hence, some action to tackle this is required.",
    "crumbs": [
      "Week 3: Rubin Causal Model (Part I)",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week3_lab.html#method-2-ancova",
    "href": "week3_lab.html#method-2-ancova",
    "title": "Lab Exercises",
    "section": "Method 2: ANCOVA",
    "text": "Method 2: ANCOVA\nMethod 2 of Schafer and Kang (2008) is based on including possible confounders as covariates (or predictors) in a model for the outcome: When there are no interactions between predictors in the model they refer to the analysis as ANCOVA, with interactions between predictors it is more generally referred to as “regression”. We will include all the covariates in our analyses below, even when they have a small standardized mean difference. In the latter case there may not be a need to correct for differences in them between the groups, but they may still account for variance in the outcome variable, and by accounting for this, we increase the power of our analysis.\nTo run an ANCOVA, we can simply run a regression model without interactions between the predictors. Our model here can be written as: \\[\n\\begin{align}DISTR.2 = &\\alpha + \\theta DIET + \\\\\n&\\beta_{1} DISTR.1 + \\beta_{2} BLACK + \\beta_{3} NBHISP + \\beta_{4} GRADE + \\beta_{5} SLFHLTH + \\\\\n&\\beta _{6} SLFWGHT + \\beta_{7} WORKSHARD + \\beta_{8} GOODQUAL + \\beta_{9} PHYSFIT + \\\\\n&\\beta_{10} PROUD + \\beta_{11} LIKESLF + \\beta_{12} ACCEPTED + \\beta_{13} FEELLOVED\n\\end{align}\n\\] Note, since the categorical variables \\(DIET\\), \\(BLACK\\) and \\(NBHISP\\) are dummy variables (they only have 2 categories), you do not need to treat them differently than the continuous covariates.\n\n\n\n\n\n\nRun the ANCOVA model (for instance using the function glm() in R), and interpret the results.\n\n\n\n\n\n\nmod_ANCOVA &lt;- glm(\n  formula = DISTR.2 ~ DIET + DISTR.1 + BLACK + NBHISP + GRADE + SLFHLTH + \n    SLFWGHT + WORKHARD + GOODQUAL + PHYSFIT + PROUD + LIKESLF + ACCEPTED + \n    FEELLOVD,\n  data = df\n)\n\nsummary(mod_ANCOVA)\n\nThe results indicate that after correcting for the effect of the covariates, the effect of the treatment (i.e., DIET) on the outcome (i.e. DISTR.2) is not significant; the parameter estimate of the ACE is \\(-0.014\\), \\(SE = 0.013\\), \\(t = -1.057\\), \\(p = .291\\).\n\n\n\nWe can extend the ANCOVA model above by incorporating product terms between the predictors:\n\nproduct between covariate and itself: quadratic effect\nproduct between two covariates: interaction\nproduct between treatment and covariates: non-parallel planes, see Figure 2 in Schafer and Kang (2008).\n\nWe will not proceed with this here—note that Schafer and Kang (2008) consider the latter option in their paper—but two additional comments are in place:\n\nAdding such product terms quickly increases the number of parameters that we need to estimate; this reduces the power (especially relevant when dealing with small sample sizes).\nBefore including interactions, one needs to center the covariates, and the interaction terms then needs to be centered again; failing to do so implies the main effect for treatment may not represent the average causal effect of treatment.",
    "crumbs": [
      "Week 3: Rubin Causal Model (Part I)",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week3_lab.html#method-3-regression-estimation",
    "href": "week3_lab.html#method-3-regression-estimation",
    "title": "Lab Exercises",
    "section": "Method 3: Regression Estimation",
    "text": "Method 3: Regression Estimation\nRegression estimation is not the same as regression analysis (which was discussed above). In regression estimation, we make actual predictions of the potential outcomes that were not observed, and use these to compute the causal effect of interest.\nTo use regression estimation, you have to:\n\nDivide the data set into those who were treated and those who were not treated.\nEstimate a regression model (with all the covariates) in each group separately.\nObtain the parameter estimates from each group; see Equations (13) and (14) in Schafer and Kang (2008).\nUse these, and the covariates to predict the potential outcomes \\(\\hat{Y}^{0\\) and \\(\\hat{Y}^1\\) for each person.\nCompute the average difference between these predicted potential outcomes.\n\nHence, this approach is a technique to impute the missing values in the data file that only contains the potential outcomes that were observed. As this is a somewhat more challenging approach, some of the code will be presented immediately, so that you can work from there to answer the questions.\nWe start with creating separate data sets for the two treatment groups, and running a regression analysis for each group separately:\n\n# Subset the data\ndf1 &lt;- subset(df, DIET==1) # Diet group\ndf0 &lt;- subset(df, DIET==0) # Non-diet group\n\n# Regression analysis with only people with X = 1\nmod_diet &lt;- glm(\n  formula = DISTR.2 ~ + DISTR.1 + as.factor(BLACK) + as.factor(NBHISP) + GRADE + \n    SLFHLTH + SLFWGHT + WORKHARD + GOODQUAL + PHYSFIT + PROUD + LIKESLF + \n    ACCEPTED + FEELLOVD,\n  data = df1\n)\n\n# Regression analysis with only people with X = 0\nmod_nondiet &lt;- glm(\n  formula = DISTR.2 ~ + DISTR.1 + as.factor(BLACK) + as.factor(NBHISP) + GRADE + \n    SLFHLTH + SLFWGHT + WORKHARD + GOODQUAL + PHYSFIT + PROUD + LIKESLF + \n    ACCEPTED + FEELLOVD,\n  data = df0\n)\n\nNow we can obtain estimates for everyone (whether we observed \\(X=1\\) or \\(X=0\\)) for the potential outcome under treatment \\(Y^1\\) and under no treatment \\(Y^1\\):\n\n# Predict outcome for all cases using parameter estimates based on only diet group\npreds_Y1 &lt;- predict(mod_diet, newdata = df)\n\n# Predict outcome for all cases using parameter estimates based on only non-diet group\npreds_Y0 &lt;- predict(mod_nondiet, newdata = df)\n\n\n\n\n\n\n\nHow can the results be used to estimate the ACE?\n\n\n\n\n\nWe now have predicted potential outcomes for everyone, and can use a t-test to compare these.\n\n# Inspect predicted potential outcomes\ndbind(preds_Y0, preds_Y1)[1:10, ]\n\n# Estimate the causal effect using a paired t-test\nt.test(preds_Y0, preds_Y1, paired = TRUE, alternative = \"two.sided\")\n\nAlthough in reality we can only ever observe one potential outcome per person, with this technique we obtained an estimate for each person’s potential outcome under treatment (\\(X=1\\)) and under no treatment (\\(X=0\\)). We can now use these estimated potential outcomes to compute the ACE by taking the mean difference.\nIt is important to realize the p-value that is obtained (and the standard error) are not correct. These are based on the assumption these are observed, rather than estimated scores.\n\n\n\nAbove we have used the predicted potential outcomes for everyone. However, one of the potential outcomes is actually observed, and we could use that one instead of the predicted potential outcomes (i.e., we use the observed fact, and predict only the counterfact). Hence, for individuals in the no treatment condition you use the observed outcome \\(Y\\) rather than predicted the predicted counterfact \\(\\hat{Y}^0\\), and for those in the treatment condition you use the observed \\(Y\\) instead of the predicted counterfact \\(\\hat{Y}^1\\).\n\npreds_Y0_onlyCounterfact &lt;- preds_Y0 # Copy predicted potential outcomes\npreds_Y0_onlyCounterfact[df$DIET==0] &lt;- df$DISTR.2[df$DIET==0] # Replace observed potential outcomes with factual\n\npreds_Y1_onlyCounterfact &lt;- preds_Y1 # Copy predicted potential outcomes\npreds_Y1_onlyCounterfact[df$DIET==1] &lt;- df$DISTR.2[df$DIET==1] # Replace observed potential outcomes with factual\n\n\n\n\n\n\n\nCheck whether this leads to a different result.\n\n\n\n\n\nAgain, we can do a paired t-test, but now with the observed and predicted potential outcomes:\n\nt.test(\n  x = preds_Y0_onlyCounterfact,\n  y = preds_Y1_onlyCounterfact, \n  paired = TRUE, \n  alternative = \"two.sided\"\n)\n\nAlthough the predicted potential outcomes and the actual observed potential outcomes are not exactly the same, using only the predicted potential outcomes (like we did first), or using a combination of predicated and observed potential outcomes (as we did here), makes virtually no difference for the estimated causal effect.",
    "crumbs": [
      "Week 3: Rubin Causal Model (Part I)",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week3_lab.html#conclusion",
    "href": "week3_lab.html#conclusion",
    "title": "Lab Exercises",
    "section": "Conclusion",
    "text": "Conclusion\nIn both Method 2 and 3 we have assumed linear relations between the covariates \\(Z\\) and the outcome \\(Y\\). This is a parametric assumption that we make in Phase III additional to the causal identification assumptions made in Phase II.",
    "crumbs": [
      "Week 3: Rubin Causal Model (Part I)",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week3_literature.html",
    "href": "week3_literature.html",
    "title": "Literature",
    "section": "",
    "text": "Please read the first part of the below article, up to the Section ``Introduction to Propensity Scores’’. Use the reading aid and reading questions to help you get a better understanding of the main points of the text.",
    "crumbs": [
      "Week 3: Rubin Causal Model (Part I)",
      "Literature"
    ]
  },
  {
    "objectID": "week3_literature.html#reading-aid",
    "href": "week3_literature.html#reading-aid",
    "title": "Literature",
    "section": "Reading Aid",
    "text": "Reading Aid\nThis article provides an overview of nine different estimation methods that can be used to estimate an average causal effect (ACE), average causal effect for the treated (ACE\\(_{1}\\)), and average causal effect for the untreated (ACE\\(_{0}\\)). Some of these estimation methods you have learned about during your bachelor, others might be new to you. Overall, the paper might appear more technical than the papers you are used to so far, with a decent amount of equations to explain the different estimation methods. Do not be discouraged, as the goal of this course is not for you to fully understand each equation. Instead, use the reading questions below to help you understand the main points of each section.\n\n“Overview”\nThe article starts with introducing the potential outcomes framework for causal inference. This introduction might explain causal inference concepts in a slightly different way; see if you can relate the information in the paper to what you have learned from earlier lectures and labs.\n\nOn page 280, the authors mention “We also make the simplifying assumption that all confounders have been measured and are available to the analyst.” Which causal identification assumption does this refer to?\n\n\n\n“The Potential-Outcomes Framework for Causal Inference”\n\nOn page 281, the authors mention the “fundamental problem of causal inference”. Explain in your own words what this means.\nOn page 282, Schafer and Kang (2008) introduce the ACE for the treated/untreated (the ACE\\(_{1}\\) and ACE\\(_{0}\\), respectively). What are the interpretations of the ACE\\(_{1}\\) and ACE\\(_{0}\\), and how are these different from the ACE?\nIn the subsection “Assumptions Needed to Estimate an ACE”, mark down which assumptions we have already discussed in previous lectures (i.e., the causal identification assumptions in Phase II), and which assumptions we have not really discussed yet.\n\n\n\n“A Simulated Observational Study”\n\nSimilar to the lab exercises of week 2, Schafer and Kang (2008) use simulations here to illustrate how the estimation methods work. Remind yourself again, why are simulations useful in this context?\nBased on the information in the text and in Table 3, see if you can draw a DAG that underlies the simulated data.\nIn the simulated data (also referred to as the “synthetic population”), what is the true ACE, ACE\\(_{1}\\), and ACE\\(_{0}\\)?\n\n\n\n“Mean Comparisons, ANCOVA, and Regression”\nTry to understand Methods 2 and 3 as much as possible. We will go over these methods in detail during the lecture, and you will practice with them in the lab.\n\nFor Method 1, which statistical test do the author use to compare the means of two groups?\nFor Method 2, think back at your bachelor courses on statistics. Which analyses that you already know would fall under Method 2?\nDescribe in your own words how Method 3 is different from Method 2.",
    "crumbs": [
      "Week 3: Rubin Causal Model (Part I)",
      "Literature"
    ]
  },
  {
    "objectID": "week4_lab_noAnswers.html",
    "href": "week4_lab_noAnswers.html",
    "title": "Lab Exercises",
    "section": "",
    "text": "In these exercises, we continue with the analyses discussed in Schafer and Kang (2008), focusing on Methods 4 to 6.",
    "crumbs": [
      "Week 4: Rubin Causal Model (Part II)",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week4_lab_noAnswers.html#setup",
    "href": "week4_lab_noAnswers.html#setup",
    "title": "Lab Exercises",
    "section": "Setup",
    "text": "Setup\nIn this practical you will make use of various R packages. If you haven’t already, install the packages tableone, MatchIt, and survey using install.packages(c(\"tableone\", \"MatchIt\", \"survey\")).\nLoad the data, which are in the data file called SchaferKangData.dat. Take a look at the data set. See Table 3 in Schafer and Kang (2008) for a description of the variables.\n\ndf &lt;- read.table(\"SchaferKangData.dat\", header = TRUE)\nhead(df, n = 10)\n\nThere are various alternative techniques that are all based on using the propensity scores: This is an individual’s probability of treatment based on their scores on the covariates, \\(P(X=1|Z=Z)\\). If we know how likely a person was to receive treatment, we can use this information to mimic a randomized controlled trial (in which everyone has the same probability of receiving treatment). Schafer and Kang (2008) consider three common techniques for this:\n\nMethod 4: Matching, in which we try to create pairs of a treated and an untreated individual that have the same propensity score.\nMethod 5: Inverse probability weighting, in which we create a pseudo-population that is balanced on the covariates.\nMethod 6: Subclassification or stratification, in which we create strata in which there are no (meaningful) differences in the covariates left.\n\nRegardless of the choise of method, we need to estimate propensity scores first.",
    "crumbs": [
      "Week 4: Rubin Causal Model (Part II)",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week4_lab_noAnswers.html#estimate-propensity-scores",
    "href": "week4_lab_noAnswers.html#estimate-propensity-scores",
    "title": "Lab Exercises",
    "section": "Estimate Propensity Scores",
    "text": "Estimate Propensity Scores\nTo compute a propensity score, run a logistic regression model in which the treatment variable \\(X\\) (which has values 0 and 1) is the outcome variable, and the covariates are the predictors. Make sure to save the probability for each person for scoring 1 on X (here DIET). You can use the glm() function from the stats package for this, setting the family argument to family = binomial().\n\n\n\n\n\n\nLogistic Propensity Score Model\n\n\n\n\n\n\n# Run logistic regression analysis\nmod_logistic &lt;- glm(\n  formula = DIET ~ DISTR.1 + as.factor(BLACK) + as.factor(NBHISP) + GRADE + \n    SLFHLTH + SLFWGHT + WORKHARD + GOODQUAL + PHYSFIT + PROUD + LIKESLF + \n    ACCEPTED + FEELLOVD,\n  family = binomial(), \n  data = df\n)\n\n# Obtain a prediction of the probability of treatment (i.e., DIET = 1) \nps &lt;- predict(mod_logistic, type = \"response\")\n\n# Add this predicted probability to the data file\ndf$ps &lt;- ps\n\n# Look at the data file \nround(df[1:10,], 2)\n\nThe last column in the data file now contains the predicted probability of being treated, based on the covariates.\n\n\n\nNow that we have the propensity scores, we should first consider the distribution of propensity scores in each of the treatment groups separately, to determine whether there is overlap between the propensity scores of the two groups.\n\n\n\n\n\n\nMake a histogram to look at this, discuss what you see, and why this is important.\n\n\n\n\n\n\n\n\n\nNow that we have determined the propensity scores and their distributions for the the treated and the non-treated overlap well, we can make use of these scores within different techniques.",
    "crumbs": [
      "Week 4: Rubin Causal Model (Part II)",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week4_lab_noAnswers.html#method-4-matching",
    "href": "week4_lab_noAnswers.html#method-4-matching",
    "title": "Lab Exercises",
    "section": "Method 4: Matching",
    "text": "Method 4: Matching\nMatching based on the propensity scores is the first technique in this category that we consider. It is based on finding people in the both treatment groups that have similar propensity scores: The idea is that these individuals are comparable on the entire set of covariates, and can thus be considered randomly assigned to the two condition.\nIn practice, this is done by taking a person from the smallest of the two groups (here the group for which DIET=1), and finding a person in the other group that is most like this person in terms of their propensity score. We can do this using the function matchit() from the R package MatchIt. Please note that this will give us slightly different results than those obtained by Schafer and Kang (2008).\nTo run the matching function, we plug in the same expression as we used above to obtain the propensity scores, and use method = \"nearest\":\n\n# Load the package\nlibrary(MatchIt)\n\n# Run matching model\nmod_matched &lt;- matchit(\n  formula = DIET ~ DISTR.1 + as.factor(BLACK) + as.factor(NBHISP) + GRADE + \n    SLFHLTH + SLFWGHT + WORKHARD + GOODQUAL + PHYSFIT + PROUD + LIKESLF + \n    ACCEPTED + FEELLOVD, \n  method = \"nearest\",  \n  data = df\n)\n\nmod_matched\n\n\n\n\n\n\n\nDescribe the information that is included in the output.\n\n\n\n\n\n\n\n\n\nThere are two useful plotting options regarding the propensity scores of our matched pairs: plot(mod_matched, type = \"jitter\") and plot(mod_matched, type = \"hist\").\n\n\n\n\n\n\nGet both plots, and describe what they represent.\n\n\n\n\n\n\n\n\n\nTo do the analysis on the matched cases only, we need to create a new data file with only the matched cases, using df_matched &lt;- match.data(mod_matched).\n\n\n\n\n\n\nCreate the Table 1 for this matched data set. What can you conclude?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInvestigate with a t-test whether the means on the outcome variable DISTR.2 differ among the matched cases.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompare this result to the mean comparison you did at the start. Explain why the mean differences that you have just determined is an estimate of the ACE\\(_{1}\\) rather than of the ACE.",
    "crumbs": [
      "Week 4: Rubin Causal Model (Part II)",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week4_lab_noAnswers.html#method-5-inverse-probability-weighting",
    "href": "week4_lab_noAnswers.html#method-5-inverse-probability-weighting",
    "title": "Lab Exercises",
    "section": "Method 5: Inverse Probability Weighting",
    "text": "Method 5: Inverse Probability Weighting\nWe can also use inverse probability weighting (IPW). In this case, the estimated propensity scores \\(\\pi_{i}\\) are used to determine the probability that an individual \\(i\\) would have received the treatment that they received:\n\nFor the treated (\\(X_{i} = 1\\)), this is simply \\(\\hat{\\pi}_{i}\\).\nFor the non-treated (\\(X_{i} = 0\\)) this is \\(1 − \\hat{\\pi}_{i}\\).\n\nWe can use these probabilities to create weights. The weight are computed by taking the inverse of the propensities (hence, the name). That way, a case that received a treatment that they was very likely to receive, will get a small weight, while a case that received a treatment that they was very unlikely to receive, will get a large weight. Thus, the inverse probability weight indicates the number of persons from the population that this person represents:\n\nFor the treated, this weight is \\(\\frac{1}{\\hat{\\pi}_{i}\\).\nFor the untreated, it is \\(\\frac{1}{1 - \\hat{\\pi}_{i}\\)\n\n\n\n\n\n\n\nCompute the ACE using IPW. More specifically, use the propensity scores that we estimated earlier, and Equation 20 of Schafer and Kang (2008).",
    "crumbs": [
      "Week 4: Rubin Causal Model (Part II)",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week4_lab_noAnswers.html#method-6-subclassification",
    "href": "week4_lab_noAnswers.html#method-6-subclassification",
    "title": "Lab Exercises",
    "section": "Method 6: Subclassification",
    "text": "Method 6: Subclassification\nSubclassification, also known as stratification and closely related to the idea of standardization in the causal inference literature, is a method that consists of creating classes (strata) based on the propensity scores. The idea is that the individuals within each stratum are rather similar with respect to their propensity score, and thus with respect to the entire set of covariates on which the propensity score is based. If the covariates are well-balanced within each stratum, this is a way to mimic an RCT within each stratum. By subsequently estimating the ACE in each stratum (using a mean comparison such as Method 1, or an ANCOVA or regression analysis such as Method 2), we can determine the causal effect for individuals who are similar with regard to the entire set of covariates (as these are used to determine the propensity scores).\n\n\n\n\n\n\nBegin with creating five strata based on the propensity scores (for instance, use the function cut() in R). Each stratum should contain 20% of the (total number of) observations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext, compute the ACE in each stratum based on the mean difference.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubsequently, you can compute the overall ACE by taking the average of the stratum-specific ACE’s (weighted by the stratum size).",
    "crumbs": [
      "Week 4: Rubin Causal Model (Part II)",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week4_lab_noAnswers.html#conclusion",
    "href": "week4_lab_noAnswers.html#conclusion",
    "title": "Lab Exercises",
    "section": "Conclusion",
    "text": "Conclusion\nThe three methods that we considered in this lab are all based on using the propensity score, that is, the probability of being treated given the covariates. The goal of these techniques is to somehow mimic the situation we get in an RCT, where the probability of treatment is independent of the covariates. In matching, this is done by creating pairs of a treated and an untreated person who have (almost) identical propensity scores, resulting in a smaller but balanced dataset; in inverse probability weighting, this is done by weighing each person’s observation by their inverse probability of received treatment, thereby creating a balanced pseudo-population; and in subclassification this is done by creating strata based on the propensity scores such that within each stratum the covariates are balanced. In each approach, we should check whether it balances the covariates, for instance, by considering the standardized mean differences. Other options for assessing covariate imbalance are described by Austin (2009).",
    "crumbs": [
      "Week 4: Rubin Causal Model (Part II)",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week5_lab.html",
    "href": "week5_lab.html",
    "title": "Lab Exercises",
    "section": "",
    "text": "The exercises in this lab will guide you through the first two phases of causal inference when studying a time-varying exposure. However, in contrast to the lectures, you will (a) work with the empirical example in VanderWeele, Jackson, and Li (2016); (b) consider a more complex (and also more realistic) causal DAG, which will influence our decisions across the phases; and (c) work with simulated data to get a better understanding of what joint effects are, and how inverse probability weighting estimation works.\nThis week is also provides important input for your assignment. Please make sure that by the end of this lab, you have read the assignment. That way, you have the opportunity to ask for clarification if anything is unclear.\n\nPhase I: Formulation\nVanderWeele, Jackson, and Li (2016) use an empirical example to illustrate the use of marginal structural models (MSMs) and inverse-probability-weighting (IPW) estimation. Re-read the section ``Results: empirical illustrations’’, and answer the below questions. Additionally, look up the empirical study upon which this example is based; You need this study to find answers to some of the questions. Finally, and perhaps most importantly, challenge yourself. Really try to come up with a comprehensive answer to the questions before you look at the answers. That way, you test if you truly grasp the contents, and whether or not you can apply it to new problems.\n\n\n\n\n\n\nFormulate the causal research question of VanderWeele, Jackson, and Li (2016) in words. In your answer, specify the target population, the exposures, the exposure contrast, and the outcome measure. Do VanderWeele, Jackson, and Li (2016) focus on specific regimes?\n\n\n\n\n\nGenerally speaking, VanderWeele, Jackson, and Li (2016) investigate the bidirectional relationship between religious service attendance and depression. More specifically, they assess the joint effect of religious service attendance in 1996 and in 2000 on depression in 2004; and the joint effect of depression in 1996 and 2000 on religious service attendance in 2004. For the rest of this exercise, we focus on religious service attendance as the exposure, and depression as the outcome. More information can be found in the study by Li et al. (2016).\nBoth VanderWeele, Jackson, and Li (2016) and Li et al. (2016) are implicit about the target population. Based on their sample, it can be inferred that their target population is female nurses across the United States. Li et al. (2016) states: “We, therefore, considered analyses both with and without participants who had a diagnosis of cardiovascular disease or cancer at baseline (n=19,803)”. They thus considered restricting their target population to only those without diagnosis of cardiovascular disease or cancer. Later, Li et al. (2016) describes how the analyses are repeated, but restricted to either only Catholics or Protestants, thereby further specifying a specific target population.\nThe outcome, depression, was defined as ``(…) either self-reported physician or clinician-diagnosed depression, or use of antidepressant medications, or depressive symptoms CESD-10 measure above 10.’’. The CESD-10 is a 10-item Likert scale screening questionnaire assessing depressive symptoms in the past week. It was measured in 2004, that is eight years after the first exposure-measurement (in 1996), and four years after the second exposure-measurement (in 2000).\nThe exposure was self-reported service attendance using the question: “How often in the past year do you attend religious services?” Answer categories were “More than once per week”, “Once per week”, “Less than once per week”, and “Never or almost never”.\nVanderWeele, Jackson, and Li (2016) and Li et al. (2016) do not express interest in any particular exposure regime. Therefore, there is also no explicit exposure contrast.\n\n\n\n\n\n\n\n\n\nIs this research question about an ACE, ACE\\(_{1}\\), or ACE\\(_{0}\\)? Reformulate the research question for those ACE\\(_{...}\\)’s left over.\n\n\n\n\n\nLi et al. (2016) research targets an average causal effect (ACE). A related ACE\\(_{1}\\) could be: “Should psychologists discourage going to religious services for those female nurses in the US that currently do attend religious services?” A related ACE\\(_{0}\\) could be: “Should psychologists encourage going to religious services at least once a week among those female nurses in the US that currently do not frequent religious services?”\n\n\n\n\n\n\n\n\n\nFormulate the marginal structural model that is used. Tip: Consider how many times the exposure was measured, and the measurement level of the exposure and outcome; also see Table 2 in VanderWeele, Jackson, and Li (2016).\n\n\n\n\n\nThe outcome is binary, and thus we need to specify a logistic marginal structural model. The exposures are categorical, containing four categories, and there are two time points at which exposures were measured. Based on this, we can infer which MSM was likely specified:\n\\[\nlogit \\; Pr[Y^{\\{A_{1}, A_{2}\\}}] = \\gamma_{0} + \\gamma_{1} A_{1, C2} + \\gamma_{2} A_{1, C3} + \\gamma_{3} A_{1,C4} + \\gamma_{4} A_{2, C2} + \\gamma_{5} A_{2, C3} + \\gamma_{6} A_{2, C4} + ...\n\\]\nThe \\(logit\\) was used here because we are dealing with dichotomous outcome. Table 2 in VanderWeele, Jackson, and Li (2016) states that the exposure category “Never” is the reference category. Therefore, \\(A_{1, C2}\\), \\(A_{1, C3}\\), and \\(A_{1, C4}\\) are dummy variables for the religious service attendance categories “&lt; 1/week” (category 2), “1/week” (category 3), and “&lt; 1/week” (category 4) in 1996, respectively. \\(A_{2, C2}\\), \\(A_{2, C3}\\), and \\(A_{2, C4}\\) are dummy variables for the religious service attendance categories “&lt; 1/week”, “1/week”, and “&lt;1/week” in 2000, respectively. \\(exp(\\gamma_{...})\\) then represent causal odds ratios. For example, \\(exp(\\gamma_{1})\\) is the causal odds ratio of depression in 2004 for religious service attendance less than once a week in 1996 versus never religious service attendance in 1996. The term \\(+ ...\\) in the MSM above represents the uncertainty around whether or not VanderWeele, Jackson, and Li (2016) additionally included baseline covariates in the MSM. Their SAS and STATA code in the online supplementary materials suggest that they did, but this is not clear from the main article.\n\n\n\n\n\n\n\n\n\nIs this a saturated MSM? If yes, what is the advantage/disadvantage of a saturated MSM? If no, what is the advantage/disadvantage of an unsaturated MSM? Tip: Think about the expected potential outcomes that exist, and compare this to the number of parameters in the MSM.\n\n\n\n\n\nA saturated MSM has as many parameters as there are potential outcome expectations. In this case, a selection of potential outcome expectations are:\n\n\\(Y^{\\{0, 0\\}}\\), representing never attending religious service in 1996 and 2000.\n\\(Y^{\\{1, 0\\}}\\), representing attending religious services &lt; 1/week in 1996, and never in 2000.\n\\(Y^{\\{2, 0\\}}\\), representing attending religious services 1/week in 1996, and never in 2000.\netc.\n\nIn total, there are 16 exposure regimes, and therefore 16 potential outcome expectations. The MSM, however, contains 7 parameters. Therefore, the MSM is not saturated, and encodes some causal assumptions. Which assumptions are encoded in this MSM? How can this MSM be adjusted to make it saturated?\nThe reason why, especially in settings with many repeated measures or with a continuous exposure, we need to work with nonsaturated MSMs is because there simply exist to many potential outcome expectations. For example, with a binary exposure and \\(k\\) number of repeated measures, there exist \\(2^{k}\\) number or potential outcome expectations, meaning that with 10 repeated measures, we would need to specify an MSM with \\(2^{10} = 1024\\) parameters.\nThe disadvantage is that nonsaturated models make assumptions which, when violated, bias our effects of interest. For the MSM from VanderWeele, Jackson, and Li (2016), we assume that the direct effect of religious service attendance in 2000 is not dependent on an individuals religious service attendance in 1996 (i.e., there is no interaction).\n\n\n\n\n\n\n\n\nReferences\n\nLi, Shanshan, Olivia I Okereke, Shun-Chiao Chang, Ichiro Kawachi, and Tyler J. VanderWeele. 2016. “Religious Service Attendance and Lower Depression Among Women: A Prospective Cohort Study.” Annals of Behavioral Medicine 50 (6): 876–84. https://doi.org/10.1007/s12160-016-9813-9.\n\n\nVanderWeele, Tyler J., John W. Jackson, and Shanshan Li. 2016. “Causal Inference and Longitudinal Data: A Case Study of Religion and Mental Health.” Social Psychiatry and Psychiatric Epidemiology 51 (11): 1457–66. https://doi.org/10.1007/s00127-016-1281-9."
  },
  {
    "objectID": "week5_literature.html",
    "href": "week5_literature.html",
    "title": "Literature",
    "section": "",
    "text": "Please read the below article. Use the reading aid and reading questions to help you get a better understanding of the main points of the text.",
    "crumbs": [
      "Week 5: MSMs and Time-Varying Exposures",
      "Literature"
    ]
  },
  {
    "objectID": "week5_literature.html#reading-aid",
    "href": "week5_literature.html#reading-aid",
    "title": "Literature",
    "section": "Reading Aid",
    "text": "Reading Aid",
    "crumbs": [
      "Week 5: MSMs and Time-Varying Exposures",
      "Literature"
    ]
  },
  {
    "objectID": "week6_lab_noAnswers.html",
    "href": "week6_lab_noAnswers.html",
    "title": "Lab Exercises",
    "section": "",
    "text": "In this lab, you practice with Phase III of a causal research project, specifically estimating causal effects of time-varying exposures using inverse probability weighting (IPW)-estimation of the parameters in a marginal structural model (MSM). To this end, we will be using simulated data again.\nWe will investigate the joint effect of a time-varying binary exposure (measured at three occasions) on a continuous distal outcome. Examples of this are the joint effect of\nIn all these cases, the exposure is a binary variable that may vary over time, and that may affect the final outcome at the end of the study directly or indirectly through shaping earlier realizations of the outcome variable.",
    "crumbs": [
      "Week 6: Estimating Causal Effects of Time-Varying Exposures",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week6_lab_noAnswers.html#structural-relations-truth",
    "href": "week6_lab_noAnswers.html#structural-relations-truth",
    "title": "Lab Exercises",
    "section": "Structural Relations (truth)",
    "text": "Structural Relations (truth)\nConsider the R code below that we will use to simulate data.\n\nN &lt;- 1000000\nC &lt;- rnorm(N)\n\ndata &lt;- data.frame(C)\n\n# Simulate data for first wave\ndata$Y1 &lt;- .4*C + rnorm(N)\ndata$X1 &lt;- rbinom(n = N, size=1, prob = plogis(C))\n\n# Simulate data for the second wave\ndata$Y2 &lt;- 0.1 * data$Y1 + 0.3 * data$X1 + rnorm(N)\ndata$X2 &lt;- rbinom(n = N, size=1, prob = \n        plogis(-.8 + 0.2 * data$Y1 + 1 * data$X1))\n\n# Simulate data for the third wave\ndata$Y3 &lt;- 0.1 * data$Y2 + 0.3 * data$X2 + \n        0.8 * data$Y1 + 0.15 * data$X1 + rnorm(N)\ndata$X3 &lt;- rbinom(n = N, size=1, prob = \n        plogis(-.8 + 0.2 * data$Y2 + 1 * data$X2 +\n            0.1 * data$Y1 + 0.8 * data$X1))\n\n# Simulate the final outcome\ndata$final.Y &lt;- 0.1 * data$Y3 + 0.3 * data$X3 + 0.8 * data$Y2 + \n            0.15 * data$X2 + rnorm(N)\n\n# Check the data file\nhead(data)\n\n           C          Y1 X1         Y2 X2          Y3 X3    final.Y\n1 -0.1177131  0.47714448  0  0.1238331  1  0.24013812  1 0.08562672\n2 -0.3903651  0.18618022  1 -0.3235108  1 -0.14280953  0 1.19822190\n3 -0.4127551 -0.07546719  1 -0.2181313  0 -1.40455489  1 1.03624585\n4 -0.5962615 -1.90517188  0 -0.0993397  0 -1.35189452  0 0.07042731\n5  0.5216229 -0.51660079  1  0.6839769  1 -0.07841375  1 1.05885288\n6 -1.5562745 -1.87834345  0  0.3761299  0 -1.34099631  0 0.59481473\n\n\n\n\n\n\n\n\nDraw the causal DAG that is encode in the R code above.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndicate which causal paths there are from \\(X_{1}\\) to \\(Y_{4}\\) and that do not go through later exposures. Based on the population parameter values in the R code to generate the data, compute what the true controlled direct effect is of \\(X_{1}\\) on \\(Y_{4}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndicate which causal paths there are from \\(X_{2}\\) to \\(Y_{4}\\) and that do not go through later exposures. Based on the population parameter values in the R code to generate the data, compute what the true controlled direct effect is of \\(X_{2}\\) on \\(Y_{4}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndicate which causal paths there are from \\(X_{3}\\) to \\(Y_{4}\\) and that do not go through later exposures. Based on the population parameter values in the R code to generate the data, compute what the true controlled direct effect is of \\(X_{3}\\) on \\(Y_{4}\\).",
    "crumbs": [
      "Week 6: Estimating Causal Effects of Time-Varying Exposures",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week6_lab_noAnswers.html#standard-linear-regression",
    "href": "week6_lab_noAnswers.html#standard-linear-regression",
    "title": "Lab Exercises",
    "section": "Standard linear regression",
    "text": "Standard linear regression\nWe want to estimate the effect of exposure over occasions 1 to 3 on an end-of-study outcome \\(Y_{4}\\). The previous measures of \\(Y\\) can be regarded as time-varying covariates.\n\n\n\n\n\n\nFor \\(Y_{1}\\), \\(Y_{2}\\), and \\(Y_{3}\\), discuss whether one should control for them, or not.\n\n\n\n\n\n\n\n\n\nBefore we consider a more sophisticated approach to this problem, we begin with considering two simpler models. Each of these is associated with a specific form of bias in estimating the causal effect of the time-varying exposure on \\(Y_{4}\\).\n\n\n\n\n\n\nRun a regression model with the time-varying covariates included.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun a regression model without the time-varying covariates.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompare the results of these two models.",
    "crumbs": [
      "Week 6: Estimating Causal Effects of Time-Varying Exposures",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week6_lab_noAnswers.html#ipw-estimation",
    "href": "week6_lab_noAnswers.html#ipw-estimation",
    "title": "Lab Exercises",
    "section": "IPW estimation",
    "text": "IPW estimation\nWe will now use the marginal structural model as described by VanderWeele, Jackson, and Li (2016).\n\n\n\n\n\n\nFirst, compute the propensity score (i.e., the probability of receiving exposure) at wave 1, wave 2, and 3 using logistic regression. For each of these, you should include all prior versions of \\(X\\) and the covariate \\(Y\\), and all time-invariant (or baseline) covariates (here \\(C\\)).\n\n\n\n\n\n\n\n\n\nNote that strictly speaking, for this particular model (as shown in the DAG), we would not need to include C to estimate the propensity scores at wave 2 and wave 3.\n\n\n\n\n\n\nExplain why not, and whether it matters that we include it here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMake histograms for the propensity scores of the treated and the untreated at wave 2 and wave 3. What does this show?\n\n\n\n\n\n\n# Plot the propensity scores at each wave\nM&lt;-matrix(c(1:3),1,3, byrow = FALSE)\nlayout(M)\n\nfor (t in 1:3)\n{   k &lt;- subset(data, select = c(paste0(\"X\", t)))\n    data.1 &lt;- data[ which(k == 1), ]\n    data.0 &lt;- data[ which(k == 0), ]\n    ps.t.1 &lt;- subset(data.1, select = c(paste0(\"ps\", t)))\n    ps.t.0 &lt;- subset(data.0, select = c(paste0(\"ps\", t)))\n    hist0 &lt;- hist(as.numeric(ps.t.1[[1]]), breaks=30, plot=FALSE)\n    hist1 &lt;- hist(ps.t.0[[1]], breaks=30, plot=FALSE)\n    title &lt;- paste0(\"Propensity scores at wave \", t)\n    plot( hist1, col=rgb(0,0,1,1/4), xlim=c(0,1), \n            xlab=\"Propensity score\", main=title)  \n    plot( hist0, col=rgb(0,1,0,1/4), xlim=c(0,1), add=T) \n}\n\nIt shows that the distribution of the propensity scores for the treated and the untreated overlap well (assumption of positivity), at each occasion.\n\n\n\nRecall that the unstabilized inverse probability weights at a specific time point are computed as \\[\nW_{it} = X_{it} \\frac{1}{P[X_{it} = 1 | \\bar{L}_{it}, \\bar{A}_{i,t - 1}]} + (1 - X_{it})\\frac{1}{(1 - P[X_{it} = 1 | \\bar{L}_{it}, \\bar{A}_{i,t - 1}])}.\n\\] From these wave-specific weights, compute the overall weight, by taking the product of the wave-specific weights.\n\n\n\n\n\n\nCompute the weights.\n\n\n\n\n\n\n\n\n\nFinally, we can run a regression model with \\(Y_{4}\\) as the outcome variable, and \\(X_1\\), \\(X_2\\), \\(X_3\\), and \\(Y_1\\) as its predictors, using the total weights computed above.\n\n\n\n\n\n\nEstimate the parameters of the MSM.",
    "crumbs": [
      "Week 6: Estimating Causal Effects of Time-Varying Exposures",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week6_lab_noAnswers.html#conclusion",
    "href": "week6_lab_noAnswers.html#conclusion",
    "title": "Lab Exercises",
    "section": "Conclusion",
    "text": "Conclusion\nCompare the results from the marginal structural model to the results from the other two models.\n\n\n\n\n\n\nWhat does this show you?\n\n\n\n\n\n\n\n\n\nWhy IPW estimation of an MSM works, and how it accounts for time-dependent confounding without blocking the relevant mediation paths, is not easy to see, or to even get some intuition for. But recall that in week 2 we learned that by using IPW, we create a balanced sample (also sometimes referred to as a pseudo population) that, within a certain level of the confounder, has an equal number of individuals in each exposure group. That implied that in this balance sample, we have \\(P[X_{it} = 1] = 0.5\\) for everyone. This balancing property of IPW is therefore a way to mimic an RCT. Balancing thus removes the arrows that point into the exposure nodes (again, as would be the case in an RCT).",
    "crumbs": [
      "Week 6: Estimating Causal Effects of Time-Varying Exposures",
      "Lab exercises"
    ]
  },
  {
    "objectID": "week6_lab_noAnswers.html#some-useful-r-packages",
    "href": "week6_lab_noAnswers.html#some-useful-r-packages",
    "title": "Lab Exercises",
    "section": "Some useful R packages",
    "text": "Some useful R packages\nFor these exercises we have made use of base R functions. This is helpful to get a better understanding of how IPW estimation actually works. However, there exist many useful packages that can help with assessing covariate (im)balance, and with creating inverse probability weights. I highly recommend the packages cobalt and WeightIt by Noah Greifer, as they have excellent documentation online that can help you use more advanced IPW-related techniques. You might consider using these packages for assignment 2.",
    "crumbs": [
      "Week 6: Estimating Causal Effects of Time-Varying Exposures",
      "Lab exercises"
    ]
  }
]