[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "description.html",
    "href": "description.html",
    "title": "Description",
    "section": "",
    "text": "In psychology and related disciplines, researchers commonly ask causal questions, albeit oftentimes implicitly so. For example, psychologists might be interested in how light physical activity might reduce depressive symptoms, if lower pay in a profession leads to fewer men choosing this profession, or if satisfaction with one’s social contacts affects self-esteem. While a randomized controlled trial (RCT) is considered the golden standard for studying such questions, there are often practical or ethical constraints that prohibit us from doing so, and researchers have to resort to using observational (i.e, nonexperimental) data.\nTo answer causal questions using observational data, we need to be extremely clear about what our causal question is, think carefully about what data we need for identifying it, and make an informed decision about which analysis techniques allows us to draw the most robust conclusions. This course provides you with an introduction to this formal causal research process. You will get acquainted with the basic building blocks of modern causal inference, learn about the steps and assumptions that need to be made throughout a study, and apply various estimation techniques to simulated and empirical (i.e., real-world) observational data. We start with introducing the causal inference building blocks like potential outcomes and directed acyclical graphs (DAGs) from a cross-sectional perspective (i.e., in which all variables have only been measured once). In the second halve of the course, we will apply these building blocks to a longitudinal settings. This allows us to ask new types of interesting causal questions, but also presents new challenges.\nThe causal inference literature provides enough material to fill an entire two-year master program, and we will only scratch the surface of the most important topics therein. Therefore, the goal of the course is (a) provide a motivation for the importance of robust causal inference in much of the modern psychological literature; (b) to introduce you to fundamental concepts of causal inference, and (c) for you to get hands-on experience with the causal research process for observational data. After this course, you can start applying some of these techniques yourself to observational data, and you can critically evaluate (implicitly) causal studies in psychology and related disciplines."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Causal Inference using Observational Data in the Social Sciences",
    "section": "",
    "text": "This is the course website for “Introduction to Causal Inference using Observational Data in the Social Sciences” (winter term 24/25), which is an optional seminar of the module “CM 6: Special topics in Psychology” of the “Psychology” master program at Humboldt-Universität. Course materials are distributed through this website, and through the accompanying Moodle course."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Syllabus",
    "section": "",
    "text": "You can find the course schedule and literature in the overview below. Lectures will take place in …. Labs take places in ….\n\n\n\n\n\n\n\n\nWeek\nWhat\nTopic\n\n\n\n\n42\nReading\n@hernan_c_word_2018\n\n\n\nLecture\nIntroduction to causal inference\n\n\n\nLab\nExercises\n\n\n43\nReading\n@schafer_average_2008 (skip Methods 7, 8, and 9)\n\n\n\nLecture\nRubin Causal Model\n\n\n\nLab\n\n\n\n44\nReading\n@roher_thinking_2018\n\n\n\nLecture\nDirected Acyclical Graphs\n\n\n\nLab\n\n\n\n45\nReading\n@vanderweele_causal_2016\n\n\n\nLecture\nMarginal Structural Models\n\n\n\nLab\n\n\n\n46\nReading\n\n\n\n\nLecture\nInverse Probability Weighting\n\n\n\nLab\n\n\n\n47\nReading\n\n\n\n\nLecture\nLinear SNMMs and G-estimation\n\n\n\nLab\n\n\n\n48\nLab\nAssignment: Preparation\n\n\n49\nLecture\nAssignment: Presentations"
  },
  {
    "objectID": "week5.html",
    "href": "week5.html",
    "title": "Joint treatment effects - Lab exercises",
    "section": "",
    "text": "This lab contains two exercises, which will guide you through the three phases of causal inferences as discussed during the lectures. The first exercise covers material discussed in the first lecture (on Monday), and the second exercise covers material discussed in the second lecture (on Wednesday). However, in contrast to the lectures, you will (a) work with the empirical example in VanderWeele, Jackson, and Li (2016); (b) consider a more complex (and also more realistic) causal DAG, which will influence our decisions across the three phases; and (c) work with simulated data to get a better understanding of what joint effects are, and how inverse probability weighting estimation works.\nThis week is also the start of the second graded group assignment. You can find the assignment on Blackboard. Please make sure that by the end of the first lab, you have read the assignment. That way, you have the opportunity to ask to lab teachers for clarification if anything is unclear."
  },
  {
    "objectID": "week5.html#structural-relations-truth",
    "href": "week5.html#structural-relations-truth",
    "title": "Joint treatment effects - Lab exercises",
    "section": "Structural relations (truth)",
    "text": "Structural relations (truth)\n\nConsider the code provided here, and draw the causal DAG that is associated with it.\n\n\nN &lt;- 1000000\nC &lt;- rnorm(N)\n\ndata &lt;- data.frame(C)\n\n# Simulate data for first wave\ndata$Y1 &lt;- .4*C + rnorm(N)\ndata$X1 &lt;- rbinom(n = N, size=1, prob = plogis(C))\n\n# Simulate data for the second wave\ndata$Y2 &lt;- 0.1 * data$Y1 + 0.3 * data$X1 + rnorm(N)\ndata$X2 &lt;- rbinom(n = N, size=1, prob = \n        plogis(-.8 + 0.2 * data$Y1 + 1 * data$X1))\n\n# Simulate data for the third wave\ndata$Y3 &lt;- 0.1 * data$Y2 + 0.3 * data$X2 + \n        0.8 * data$Y1 + 0.15 * data$X1 + rnorm(N)\ndata$X3 &lt;- rbinom(n = N, size=1, prob = \n        plogis(-.8 + 0.2 * data$Y2 + 1 * data$X2 +\n            0.1 * data$Y1 + 0.8 * data$X1))\n\n# Simulate the final outcome\ndata$final.Y &lt;- 0.1 * data$Y3 + 0.3 * data$X3 + 0.8 * data$Y2 + \n            0.15 * data$X2 + rnorm(N)\n\n# Check the data file\nhead(data)\n\n            C         Y1 X1         Y2 X2          Y3 X3    final.Y\n1 -1.40675231  2.4230867  1  1.4509665  1  2.36361839  1  2.2381028\n2 -0.39197712 -0.3915516  1  0.3166732  0  0.33184267  0  2.1755570\n3  0.99282297  0.7441825  1 -0.1793503  1  0.06586861  1 -0.6411101\n4 -0.08458058  0.5457411  0  0.2642694  1  1.76237329  1  0.9589737\n5  2.03165819  0.2382333  1  0.7156746  1 -0.92780462  0 -0.4561419\n6  0.46247749  1.3485845  1  1.4051926  1  1.83443314  1  1.9172957\n\n\n\n\n\n\n\n\nAnswer 1: Causal DAG\n\n\n\n\n\n\n\n\nCausal DAG based on the R code.\n\n\nThis figure does not depict linear or logistic regressions per se, but instead visualizes the causal (structural) relations between the variables. However, since we generated the data ourselves, we know that these relationships are in fact linear and logistic.\n\n\n\n\nIndicate which causal paths there are from \\(X_{1}\\) to \\(Y_{4}\\) and that do not go through later exposures. Based on the population parameter values in the R code to generate the data, compute what the true controlled direct effect is of \\(X_{1}\\) on \\(Y_{4}\\).\n\n\n\n\n\n\n\nAnswer 2: True CDE \\(X_{1} \\rightarrow Y_{4}\\)\n\n\n\n\n\n\n# X1 -&gt; Y2 -&gt; Y3 -&gt; Y.final\n0.3*0.1*0.1\n\n[1] 0.003\n\n# X1 -&gt; Y2 -&gt; Y.final\n0.3*0.8\n\n[1] 0.24\n\n# X1 -&gt; Y3 -&gt; Y.final\n0.15*0.1\n\n[1] 0.015\n\n# Total CDE of X1 on Y4\n0.3*0.1*0.1 + 0.3*0.8 + 0.15*0.1\n\n[1] 0.258\n\n\n\n\n\n\nIndicate which causal paths there are from \\(X_{2}\\) to \\(Y_{4}\\) and that do not go through later exposures. Based on the population parameter values in the R code to generate the data, compute what the true controlled direct effect is of \\(X_{2}\\) on \\(Y_{4}\\).\n\n\n\n\n\n\n\nAnswer 3: True CDE \\(X_{2} \\rightarrow Y_{4}\\)\n\n\n\n\n\n\n# X2 -&gt; Y3 -&gt; Y.final\n0.3*0.1\n\n[1] 0.03\n\n# X2 -&gt; Y.final\n0.15\n\n[1] 0.15\n\n# Total CDE of X2 on Y4\n0.3*0.1 + 0.15\n\n[1] 0.18\n\n\n\n\n\n\nIndicate which causal paths there are from \\(X_{3}\\) to \\(Y_{4}\\) and that do not go through later exposures. Based on the population parameter values in the R code to generate the data, compute what the true controlled direct effect is of \\(X_{3}\\) on \\(Y_{4}\\).\n\n\n\n\n\n\n\nAnswer 4: True CDE \\(X_{3} \\rightarrow Y_{4}\\)\n\n\n\n\n\n\n# X3 -&gt; Y.final\n0.3\n\n[1] 0.3\n\n# Total CDE X3 on Y4\n0.3\n\n[1] 0.3"
  },
  {
    "objectID": "week5.html#standard-linear-regression",
    "href": "week5.html#standard-linear-regression",
    "title": "Joint treatment effects - Lab exercises",
    "section": "Standard linear regression",
    "text": "Standard linear regression\nWe want to estimate the effect of exposure over occasions 1 to 3 on an end-of-study outcome \\(Y_{4}\\). The previous measures of \\(Y\\) can be regarded as time-varying covariates.\n\nFor \\(Y_{1}\\), \\(Y_{2}\\), and \\(Y_{3}\\), discuss whether one should control for them, or not.\n\n\n\n\n\n\n\nAnswer 5\n\n\n\n\n\nVariable \\(Y_{1}\\) is only a confounder between exposure and the outcome; hence, we should control for it.\nThe variable \\(Y_{2}\\) has two roles in the causal structure. It is a confounder of the relation between \\(X_{3}\\) and \\(Y_{4}\\): \\(X_{3} \\leftarrow Y_{2} \\rightarrow Y_{3} \\rightarrow Y_{4}\\). This would imply we should control for it to avoid confounder bias. It is also a mediator for the effect of \\(X_{1}\\) on \\(Y_{4}\\): \\(X_{1} \\rightarrow Y_{2} \\rightarrow Y_{3} \\rightarrow Y_{4}\\) and \\(X_{1} \\rightarrow Y_{2} \\rightarrow X_{3} \\rightarrow Y_{4}\\). This would imply we should not control for it, to avoid overcontrol bias (that is, bias that arises when we block an indirect path from the cause to the effect). Controlling for \\(Y_{2}\\) would remove part of the total causal effect \\(X_{1}\\) has on \\(Y_{4}\\). This is a catch-22.\nThe variable \\(Y_{3}\\) is only a mediator in this model: \\(X_{1} \\rightarrow Y_{2} \\rightarrow Y_{3} \\rightarrow Y_{4}\\); \\(X_{1} \\rightarrow X_2 \\rightarrow Y_{3} \\rightarrow Y_{4}\\); \\(X_{1} \\rightarrow Y_{3} \\rightarrow Y_{4}\\); \\(X_{2} \\rightarrow Y_{3} \\rightarrow Y_{4}\\). So, including \\(Y_{3}\\) as a covariate will lead to overcontrol bias in estimating the controlled direct effects of \\(X_{1}\\) and \\(X_{2}\\).\n\n\n\n\nBefore we consider a more sophisticated approach to this problem, we begin with considering two simpler models. Each of these is associated with a specific form of bias in estimating the causal effect of the time-varying exposure on \\(Y_{4}\\). Run a regression model with the time-varying covariates included.\n\n\n\n\n\n\n\nAnswer 6: Standard regression 1\n\n\n\n\n\n\nout1 &lt;- glm(final.Y ~ C + X1 + X2 + X3 + Y1 + Y2 + Y3, data=data)\nsummary(out1)\n\n\nCall:\nglm(formula = final.Y ~ C + X1 + X2 + X3 + Y1 + Y2 + Y3, data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-4.8950  -0.6758   0.0006   0.6737   4.9796  \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.004e-04  1.760e-03   0.171   0.8645    \nC            9.782e-05  1.171e-03   0.084   0.9334    \nX1           4.374e-03  2.314e-03   1.890   0.0587 .  \nX2           1.478e-01  2.177e-03  67.889   &lt;2e-16 ***\nX3           3.001e-01  2.148e-03 139.751   &lt;2e-16 ***\nY1          -5.358e-05  1.291e-03  -0.042   0.9669    \nY2           7.998e-01  1.011e-03 791.064   &lt;2e-16 ***\nY3           9.968e-02  1.001e-03  99.592   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 1.001978)\n\n    Null deviance: 1805402  on 999999  degrees of freedom\nResidual deviance: 1001970  on 999992  degrees of freedom\nAIC: 2839863\n\nNumber of Fisher Scoring iterations: 2\n\n\nIn this model, we have the problem of over control bias due to controlling for mediators \\(Y_{2}\\) and \\(Y_{3}\\), which blocks causal paths from the time-varying exposure to the outcome.\n\n\n\n\nRun a regression model without the time-varying covariates.\n\n\n\n\n\n\n\nAnswer 7: Standard regression 2\n\n\n\n\n\n\nout2 &lt;- glm(final.Y ~ C + X1 + X2 + X3 + Y1, data=data)\nsummary(out2)\n\n\nCall:\nglm(formula = final.Y ~ C + X1 + X2 + X3 + Y1, data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-6.5845  -0.8681  -0.0009   0.8694   6.7382  \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.049595   0.002265 -21.899   &lt;2e-16 ***\nC           -0.000646   0.001507  -0.429    0.668    \nX1           0.229977   0.002953  77.872   &lt;2e-16 ***\nX2           0.141297   0.002776  50.904   &lt;2e-16 ***\nX3           0.460214   0.002753 167.178   &lt;2e-16 ***\nY1           0.157142   0.001297 121.178   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 1.660459)\n\n    Null deviance: 1805402  on 999999  degrees of freedom\nResidual deviance: 1660449  on 999994  degrees of freedom\nAIC: 3344979\n\nNumber of Fisher Scoring iterations: 2\n\n\nThis model results in confounder bias because it fails to control for confounders \\(Y_{1}\\) and \\(Y_{2}\\). Note that \\(Y_{3}\\) is not a counfounder in this model, so omitting it is not associated with confounder bias.\n\n\n\n\nCompare the results of these two models.\n\n\n\n\n\n\n\nAnswer 8: Standard regression comparison\n\n\n\n\n\nThe results for the effect of \\(X_1\\) to \\(X_3\\) on \\(Y_{4}\\) are quite different across these two models, and they also deviate from the actual effects that we computed based on the truth above:\n\n\n\nExposure\nModel 1\nModel 2\nTruth\n\n\n\n\n\\(X_{1}\\)\n-0.00\n0.22\n0.258\n\n\n\\(X_{2}\\)\n0.15\n0.14\n0.180\n\n\n\\(X_{3}\\)\n0.30\n0.46\n0.300\n\n\n\nWe know that neither model is correct: Model 1 blocks causal paths through \\(Y_2\\) and \\(Y_3\\) (overcontrol bias), while model 2 fails to account for confounding due to \\(Y_1\\) and \\(Y_2\\)."
  },
  {
    "objectID": "week5.html#ipw-estimation",
    "href": "week5.html#ipw-estimation",
    "title": "Joint treatment effects - Lab exercises",
    "section": "IPW estimation",
    "text": "IPW estimation\n\nWe will now use the marginal structural model as described by VanderWeele, Jackson, and Li (2016). First, compute the propensity score (i.e., the probability of receiving exposure) at wave 1, wave 2, and 3 using logistic regression. For each of these, you should include all prior versions of \\(X\\) and the covariate \\(Y\\), and all time-invariant (or baseline) covariates (here \\(C\\)).\n\n\n\n\n\n\n\nAnswer 9: Propensity scores using logistic regression\n\n\n\n\n\n\n# Compute the propensity scores at wave 1 \nres.X1 &lt;- glm(X1 ~  C, family = binomial(), data = data)\nps1 &lt;- predict(res.X1, type = \"response\")\ndata$ps1 &lt;- ps1\n\n# Compute the propensity scores at wave 2 \nres.X2 &lt;- glm(X2 ~  C + X1 + Y1, family = binomial(), data = data)\nps2 &lt;- predict(res.X2, type = \"response\")\ndata$ps2 &lt;- ps2\n\n# Compute the propensity scores at wave 3 \nres.X3 &lt;- glm(X3 ~ C + X1 + X2 + Y1 + Y2, family = binomial(), data = data)\nps3 &lt;- predict(res.X3, type = \"response\")\ndata$ps3 &lt;- ps3\n\n\n\n\n\nNote that strictly speaking, for this particular model (as shown in the DAG), we would not need to include C to estimate the propensity scores at wave 2 and wave 3. Explain why not, and whether it matters that we include it here.\n\n\n\n\n\n\n\nAnswer 10\n\n\n\n\n\n\\(C\\) only has direct effects on X1 and Y1; since both are included as predictors for the propensity scores at wave 2 and wave 3, the effect of \\(C\\) is already controlled for then. However, it does not create a problem to include it; it does not block a indirect path that we would want to remain open (i.e., it is not a mediator), nor does it open a path that should remain closed (i.e., it is not a collider). Including or not including it for ps2 and ps3 does not make a difference.\n\n\n\n\nMake histograms for the propensity scores of the treated and the untreated at wave 2 and wave 3. What does this show?\n\n\n\n\n\n\n\nAnswer 11\n\n\n\n\n\n\n# Plot the propensity scores at each wave\nM&lt;-matrix(c(1:3),1,3, byrow = FALSE)\nlayout(M)\n\nfor (t in 1:3)\n{   k &lt;- subset(data, select = c(paste0(\"X\", t)))\n    data.1 &lt;- data[ which(k == 1), ]\n    data.0 &lt;- data[ which(k == 0), ]\n    ps.t.1 &lt;- subset(data.1, select = c(paste0(\"ps\", t)))\n    ps.t.0 &lt;- subset(data.0, select = c(paste0(\"ps\", t)))\n    hist0 &lt;- hist(as.numeric(ps.t.1[[1]]), breaks=30, plot=FALSE)\n    hist1 &lt;- hist(ps.t.0[[1]], breaks=30, plot=FALSE)\n    title &lt;- paste0(\"Propensity scores at wave \", t)\n    plot( hist1, col=rgb(0,0,1,1/4), xlim=c(0,1), \n            xlab=\"Propensity score\", main=title)  \n    plot( hist0, col=rgb(0,1,0,1/4), xlim=c(0,1), add=T) \n}\n\n\n\n\nIt shows that the distribution of the propensity scores for the treated and the untreated overlap well (assumption of positivity), at each occasion.\n\n\n\n\nNext, compute the unstabilized inverse probability weights at wave 1, 2, and 3. Recall that these are computed as \\[\nW_{it} = X_{it} \\frac{1}{P[X_{it} = 1 | \\bar{L}_{it}, \\bar{A}_{i,t - 1}]} + (1 - X_{it})\\frac{1}{(1 - P[X_{it} = 1 | \\bar{L}_{it}, \\bar{A}_{i,t - 1}])}\n\\] From these wave-specific weights, compute the overall weight, by taking the product of the wave-specific weights.\n\n\n\n\n\n\n\nAnswer 12: Compute weights\n\n\n\n\n\n\n# Compute the inverse probability weights at waves 1, 2 and 3\ndata$ipw1 &lt;- ifelse(data$X1==1, 1/data$ps1, 1/(1-data$ps1))\ndata$ipw2 &lt;- ifelse(data$X2==1, 1/data$ps2, 1/(1-data$ps2))\ndata$ipw3 &lt;- ifelse(data$X3==1, 1/data$ps3, 1/(1-data$ps3))\n\n# Compute the total ipw\ndata$ipw.123 &lt;- data$ipw1 * data$ipw2 * data$ipw3 \n\n\n\n\n\nFinally, we can run a regression model with \\(Y_{4}\\) as the outcome variable, and \\(X_1\\), \\(X_2\\), \\(X_3\\), and \\(Y_1\\) as its predictors, using the total weights computed above.\n\n\n\n\n\n\n\nAnswer 13: Effect estimation\n\n\n\n\n\n\n# Regression with inverse probability weighting with X1, X2, X3 and Y1 as its predictors\nout3 &lt;- glm(final.Y ~ X1 + X2 + X3 + Y1, \n        weights = ipw.123, data=data)\n\n# Another regression with inverse probability weighting which now also includes the baseline confounder (just \n# for comparison)\nout4 &lt;- glm(final.Y ~ C + X1 + X2 + X3 + Y1, \n        weights = ipw.123, data=data)\n\n# Another regression with inverse probability weighting which includes C but not Y1\nout5 &lt;- glm(final.Y ~ C + X1 + X2 + X3, \n        weights = ipw.123, data=data)\n\nsummary(out3)\n\n\nCall:\nglm(formula = final.Y ~ X1 + X2 + X3 + Y1, data = data, weights = ipw.123)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-39.778   -2.158    0.001    2.167   36.911  \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.0009334  0.0025792  -0.362    0.717    \nX1           0.2615095  0.0025794 101.384   &lt;2e-16 ***\nX2           0.1780966  0.0025794  69.046   &lt;2e-16 ***\nX3           0.3010356  0.0025794 116.708   &lt;2e-16 ***\nY1           0.1600538  0.0011989 133.501   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 13.30561)\n\n    Null deviance: 13925010  on 999999  degrees of freedom\nResidual deviance: 13305547  on 999995  degrees of freedom\nAIC: 3556266\n\nNumber of Fisher Scoring iterations: 2\n\nsummary(out4)\n\n\nCall:\nglm(formula = final.Y ~ C + X1 + X2 + X3 + Y1, data = data, weights = ipw.123)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-39.738   -2.158    0.001    2.167   36.862  \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.0009286  0.0025792  -0.360    0.719    \nC           -0.0017017  0.0013890  -1.225    0.221    \nX1           0.2615056  0.0025794 101.382   &lt;2e-16 ***\nX2           0.1780967  0.0025794  69.046   &lt;2e-16 ***\nX3           0.3010342  0.0025794 116.707   &lt;2e-16 ***\nY1           0.1606410  0.0012912 124.416   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 13.30561)\n\n    Null deviance: 13925010  on 999999  degrees of freedom\nResidual deviance: 13305527  on 999994  degrees of freedom\nAIC: 3556267\n\nNumber of Fisher Scoring iterations: 2\n\nsummary(out5)\n\n\nCall:\nglm(formula = final.Y ~ C + X1 + X2 + X3, data = data, weights = ipw.123)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-41.881   -2.173    0.001    2.183   37.129  \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.001428   0.002599  -0.549    0.583    \nC            0.062452   0.001300  48.051   &lt;2e-16 ***\nX1           0.262338   0.002599 100.927   &lt;2e-16 ***\nX2           0.178429   0.002599  68.645   &lt;2e-16 ***\nX3           0.300823   0.002599 115.733   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 13.51156)\n\n    Null deviance: 13925010  on 999999  degrees of freedom\nResidual deviance: 13511489  on 999995  degrees of freedom\nAIC: 3571625\n\nNumber of Fisher Scoring iterations: 2\n\n\nIt shows that whether the baseline covariate \\(C\\) is included or not, makes no difference. This is because it has already been accounted for when computing the inverse probability weights. Furthermore, when considering the parameter estimates here it can be seen these are in fact very close to the true effects, which were computed at the start based on the parameter values that were used to simulate the data (i.e., the Truth)."
  },
  {
    "objectID": "week5.html#conclusion",
    "href": "week5.html#conclusion",
    "title": "Joint treatment effects - Lab exercises",
    "section": "Conclusion",
    "text": "Conclusion\n\nCompare the results from the marginal structural model to the results from the other two models. What does this show you?\n\n\n\n\n\n\n\nAnswer 14: Conclusion\n\n\n\n\n\n\n\n\nExposure\nModel 1\nModel 2\nModel 3\nTruth\n\n\n\n\n\\(X_{1}\\)\n-0.00\n0.22\n0.26\n0.258\n\n\n\\(X_{2}\\)\n0.15\n0.14\n0.18\n0.180\n\n\n\\(X_{3}\\)\n0.30\n0.46\n0.30\n0.300\n\n\n\nModel 1 includes \\(Y_2\\) and \\(Y_3\\) as a covariate, and thus leads to overcontrol bias when estimating the effects of \\(X_1\\) and \\(X_2\\). It only shows the direct effects of the time-varying exposure, not the indirect effects.\nModel 2 does not include \\(Y_2\\) and \\(Y_3\\) in any manner. Therefore, this model leads to confounder bias, since \\(Y_2\\) is a confounder for the \\(X_3 \\rightarrow Y_{4}\\) relation.\nModel 3 is used to account for confounders, without blocking indirect paths. Hence, it should inform us on the joint exposure effect of the time-varying exposure. Its estimates are very close to the true values (based on the model parameters that we used to simulated the data with).\n\n\n\nWhy IPW estimation of an MSM works, and how it accounts for time-dependent confounding without blocking the relevant mediation paths, is not easy to see, or to even get some intuition for. But recall that in week 2 we learned that by using IPW, we create a balanced sample (also sometimes referred to as a pseudo population) that, within a certain level of the confounder, has an equal number of individuals in each exposure group. That implied that in this balance sample, we have \\(P[X_{it} = 1] = 0.5\\) for everyone. This balancing property of IPW is therefore a way to mimic an RCT. Balancing thus removes the arrows that point into the exposure nodes (again, as would be the case in an RCT)."
  },
  {
    "objectID": "week5.html#some-useful-r-packages",
    "href": "week5.html#some-useful-r-packages",
    "title": "Joint treatment effects - Lab exercises",
    "section": "Some useful R packages",
    "text": "Some useful R packages\nFor these exercises we have made use of base R functions. This is helpful to get a better understanding of how IPW estimation actually works. However, there exist many useful packages that can help with assessing covariate (im)balance, and with creating inverse probability weights. I highly recommend the packages cobalt and WeightIt by Noah Greifer, as they have excellent documentation online that can help you use more advanced IPW-related techniques. You might consider using these packages for assignment 2."
  }
]